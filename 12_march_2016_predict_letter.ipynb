{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'ORI'\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "# I should learn how to load libraries in a more elegant way\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(r'C:\\Users\\ORI\\Documents\\IDC-non-sync\\Thesis\\PythonApplication1\\OriKerasExtension')\n",
    "#import OriKerasExtension\n",
    "import ThesisHelper\n",
    "#reload(OriKerasExtension)\n",
    "reload(ThesisHelper)\n",
    "from   ThesisHelper import LoadSingleSubjectPython, readCompleteMatFile, ExtractDataVer4\n",
    "import P300Prediction\n",
    "reload(P300Prediction)\n",
    "from P300Prediction import accuracy_by_repetition, create_target_table\n",
    "\n",
    "\n",
    "sys.path.append(r'C:\\Users\\ORI\\Documents\\IDC-non-sync\\Thesis\\PythonApplication1\\OriKerasExtension')\n",
    "#import OriKerasExtension\n",
    "import ThesisHelper\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import P300Prediction\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "#reload(OriKerasExtension)\n",
    "reload(ThesisHelper)\n",
    "from ThesisHelper import LoadSingleSubjectPython, readCompleteMatFile, ExtractDataVer4\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "reload(P300Prediction)\n",
    "from P300Prediction import accuracy_by_repetition, create_target_table\n",
    "\n",
    "\n",
    "# [all_target, all_non_target] = LoadSingleSubjectPython(r'C:\\Users\\ORI\\Documents\\Thesis\\dataset_all\\RSVP_Color116msVPfat.mat')\n",
    "\n",
    "\n",
    "\n",
    "# all_samples = np.vstack((all_target,all_non_target))\n",
    "\n",
    "\n",
    "# '''\n",
    "# Create the tagging column\n",
    "# '''\n",
    "# all_tags = np.vstack((np.ones((all_target.shape[0],1)), np.zeros((all_non_target.shape[0],1))))\n",
    "\n",
    "\n",
    "\n",
    "# from OriKerasExtension.OriKerasExtension import DebugLSTM\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from  keras.regularizers import WeightRegularizer\n",
    "\n",
    "'''\n",
    "define the neural network model:\n",
    "'''\n",
    "\n",
    "\n",
    "def create_compile_cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    number_of_time_stamps = 20\n",
    "    number_of_out_channels = 10\n",
    "    number_of_in_channels = 55\n",
    "    length_of_time_axe_mask = 10\n",
    "\n",
    "    model.add(Convolution2D(nb_filter=10,\n",
    "                            nb_col=number_of_out_channels,\n",
    "                            nb_row=1,\n",
    "                            input_shape=(1, number_of_time_stamps, number_of_in_channels),\n",
    "                            border_mode='same',\n",
    "                            init='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(1, number_of_in_channels)))\n",
    "    model.add(\n",
    "        Convolution2D(nb_filter=number_of_out_channels, nb_row=6, nb_col=1, border_mode='same', init='glorot_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(20, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "    return model\n",
    "\n",
    "def create_compile_lstm_model():\n",
    "\n",
    "    '''\n",
    "    define the neural network model:\n",
    "    '''\n",
    "    model_lstm = Sequential()\n",
    "\n",
    "    model_lstm.add(LSTM(input_dim=55, output_dim=20,return_sequences=True))\n",
    "    model_lstm.add(Dropout(0.3))\n",
    "    model_lstm.add(LSTM(input_dim=20, output_dim=20,return_sequences=False))\n",
    "    model_lstm.add(Dense(2, W_regularizer=l2(0.06)))\n",
    "    model_lstm.add(Activation('softmax'))\n",
    "    model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "def create_compile_lstm_model_letter():\n",
    "\n",
    "    '''\n",
    "    define the neural network model:\n",
    "    '''\n",
    "    model_lstm = Sequential()\n",
    "\n",
    "    model_lstm.add(LSTM(input_dim=55, output_dim=20,return_sequences=True))\n",
    "#     model_lstm.add(Dropout(0.01))\n",
    "    model_lstm.add(LSTM(input_dim=20, output_dim=20,return_sequences=True))\n",
    "#     model_lstm.add(Dropout(0.01))\n",
    "    model_lstm.add(LSTM(input_dim=20, output_dim=20,return_sequences=False))\n",
    "    model_lstm.add(Dense(31, W_regularizer=l2(0.0006)))\n",
    "    model_lstm.add(Activation('softmax'))\n",
    "    model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "\n",
    "def create_compile_dense_model():\n",
    "\n",
    "    '''\n",
    "    define the neural network model:\n",
    "    '''\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(keras.layers.core.Flatten(input_shape=(55,100)))\n",
    "    model_lstm.add(Dense(input_dim=55*100, output_dim=30 , W_regularizer=l2(0.06)))\n",
    "    model_lstm.add(Activation('tanh'))\n",
    "    model_lstm.add(Dense(2))\n",
    "    model_lstm.add(Activation('softmax'))\n",
    "    model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "def create_small_compile_dense_model():\n",
    "\n",
    "    '''\n",
    "    define the neural network model:\n",
    "    '''\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(keras.layers.core.Flatten(input_shape=(55,25)))\n",
    "    model_lstm.add(Dense(input_dim=55*25, output_dim=20 ))\n",
    "    model_lstm.add(Dropout(0.3))\n",
    "    model_lstm.add(Activation('tanh'))\n",
    "    model_lstm.add(Dense(output_dim=20 , W_regularizer=l2(0.06)))\n",
    "    model_lstm.add(Activation('tanh'))\n",
    "    model_lstm.add(Dense(2))\n",
    "    model_lstm.add(Activation('softmax'))\n",
    "    model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "# def down_sample_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_evaluation_data(gcd_res, down_samples_param):\n",
    "#     gcd_res = readCompleteMatFile(file_name)\n",
    "    data_for_eval = ExtractDataVer4(gcd_res['all_relevant_channels'], gcd_res['marker_positions'], gcd_res['target'],\n",
    "                                    -200, 800)\n",
    "    # print  data_for_eval\n",
    "\n",
    "    temp_data_for_eval = downsample_data(data_for_eval[0], data_for_eval[0].shape[1], down_samples_param)\n",
    "\n",
    "    test_data_gcd, test_target_gcd = temp_data_for_eval[gcd_res['train_mode'] != 1], data_for_eval[1][\n",
    "        gcd_res['train_mode'] != 1]\n",
    "    return test_data_gcd, test_target_gcd\n",
    "\n",
    "\n",
    "def downsample_data(data, number_of_original_samples, down_samples_param):\n",
    "\n",
    "\n",
    "    new_number_of_time_stamps = number_of_original_samples / down_samples_param\n",
    "\n",
    "\n",
    "    # print  data_for_eval\n",
    "    temp_data_for_eval = np.zeros((data.shape[0], new_number_of_time_stamps, data.shape[2]))\n",
    "\n",
    "    for new_i, i in enumerate(range(0, number_of_original_samples, down_samples_param)):\n",
    "        temp_data_for_eval[:, new_i, :] = np.mean(data[:, range(i, (i + down_samples_param)), :], axis=1)\n",
    "    return temp_data_for_eval\n",
    "\n",
    "\n",
    "def create_train_data(gcd_res, down_samples_param):\n",
    "    all_positive_train = []\n",
    "    all_negative_train = []\n",
    "\n",
    "    last_time_stamp = 800\n",
    "    fist_time_stamp = -200\n",
    "\n",
    "\n",
    "    data_for_eval = ExtractDataVer4(gcd_res['all_relevant_channels'], gcd_res['marker_positions'],\n",
    "                                    gcd_res['target'], fist_time_stamp, last_time_stamp)\n",
    "    \n",
    "    print data_for_eval[0].shape\n",
    "    temp_data_for_eval = downsample_data(data_for_eval[0],data_for_eval[0].shape[1], down_samples_param)\n",
    "\n",
    "    all_data = temp_data_for_eval[np.all([gcd_res['train_mode'] != 3], axis=0)]\n",
    "    categorical_tags = to_categorical(gcd_res['stimulus'][gcd_res['train_mode'] != 3])\n",
    "    indexes = range(len(categorical_tags))\n",
    "    print \"len(categorical_tags) {0}\".format(len(categorical_tags))\n",
    "\n",
    "    shuffeled_samples, suffule_tags = (all_data, categorical_tags)\n",
    "    return shuffeled_samples, suffule_tags\n",
    "\n",
    "def create_letter_test_data(gcd_res, down_samples_param):\n",
    "    all_positive_train = []\n",
    "    all_negative_train = []\n",
    "\n",
    "    last_time_stamp = 800\n",
    "    fist_time_stamp = -200\n",
    "\n",
    "\n",
    "    data_for_eval = ExtractDataVer4(gcd_res['all_relevant_channels'], gcd_res['marker_positions'],\n",
    "                                    gcd_res['target'], fist_time_stamp, last_time_stamp)\n",
    "    \n",
    "    print data_for_eval[0].shape\n",
    "    temp_data_for_eval = downsample_data(data_for_eval[0],data_for_eval[0].shape[1], down_samples_param)\n",
    "\n",
    "    all_data = temp_data_for_eval[np.all([gcd_res['train_mode'] == 3], axis=0)]\n",
    "    categorical_tags = to_categorical(gcd_res['stimulus'][gcd_res['train_mode'] == 3])\n",
    "    indexes = range(len(categorical_tags))\n",
    "    print \"len(categorical_tags) {0}\".format(len(categorical_tags))\n",
    "\n",
    "    shuffeled_samples, suffule_tags = (all_data, categorical_tags)\n",
    "    return shuffeled_samples, suffule_tags\n",
    "\n",
    "\n",
    "def create_data_for_compare_by_repetition(file_name):\n",
    "    gcd_res = readCompleteMatFile(file_name)\n",
    "    sub_gcd_res = dict(train_trial=gcd_res['train_trial'][gcd_res['train_mode'] != 1],\n",
    "                       train_block=gcd_res['train_block'][gcd_res['train_mode'] != 1],\n",
    "                       stimulus=gcd_res['stimulus'][gcd_res['train_mode'] != 1])\n",
    "    return sub_gcd_res\n",
    "\n",
    "#shuffeled_samples, suffule_tags = create_train_data(file_name=None, down_samples_param=5)\n",
    "# shuffeled_samples, suffule_tags = create_train_data(file_name=None, down_samples_param=20)\n",
    "# original_weights_mlp = model_mlp.get_weights()\n",
    "\n",
    "data_set_locations = [\"RSVP_Color116msVPicr.mat\",\n",
    "                      \"RSVP_Color116msVPpia.mat\",\n",
    "                      \"RSVP_Color116msVPfat.mat\",\n",
    "                      \"RSVP_Color116msVPgcb.mat\",\n",
    "                      \"RSVP_Color116msVPgcc.mat\",\n",
    "                      \"RSVP_Color116msVPgcd.mat\",\n",
    "                      \"RSVP_Color116msVPgcf.mat\",\n",
    "                      \"RSVP_Color116msVPgcg.mat\",\n",
    "                      \"RSVP_Color116msVPgch.mat\",\n",
    "                      \"RSVP_Color116msVPiay.mat\",\n",
    "                      \"RSVP_Color116msVPicn.mat\"];\n",
    "\n",
    "# data_set_locations = [\"RSVP_Color116msVPgcd.mat\"]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for subject_name in data_set_locations:\n",
    "#     file_name = r'C:\\Users\\ORI\\Documents\\Thesis\\dataset_all\\{0}'.format(subject_name)\n",
    "#     gcd_res = readCompleteMatFile(file_name)\n",
    "\n",
    "def print_true_vs_predict(true_val, predicted_val):\n",
    "    import matplotlib.cm as cm        \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(true_val[:,1].reshape(-1,30), cmap=cm.Greys_r, interpolation='none', aspect='auto')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(predicted_val[:,1].reshape(-1,30), cmap=cm.Greys_r, interpolation='none', aspect='auto')\n",
    "    plt.show()\n",
    "\n",
    "# def calculate_proximity_to_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18600L, 200L, 55L)\n",
      "len(categorical_tags) 7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[  428.875,   146.625,   -57.5  , ...,   618.   ,  -366.875,\n",
       "           -458.875],\n",
       "         [  352.375,    95.75 ,  -149.25 , ...,   658.75 ,  -275.125,\n",
       "           -351.875],\n",
       "         [  514.125,   228.125,    13.75 , ...,   569.75 ,  -318.5  ,\n",
       "           -425.625],\n",
       "         ..., \n",
       "         [  935.5  ,   671.625,   440.125, ...,   525.   ,  -352.625,\n",
       "           -597.25 ],\n",
       "         [  948.   ,   701.125,   447.   , ...,   475.25 ,  -473.125,\n",
       "           -711.75 ],\n",
       "         [ 1068.875,   829.5  ,   591.375, ...,   530.   ,  -540.125,\n",
       "           -746.625]],\n",
       " \n",
       "        [[  551.875,   302.25 ,    66.625, ...,   541.875,  -485.875,  -521.   ],\n",
       "         [  752.125,   510.75 ,   280.5  , ...,   636.   ,  -275.375,\n",
       "           -384.375],\n",
       "         [  827.375,   575.875,   342.25 , ...,   672.   ,  -319.125,\n",
       "           -499.25 ],\n",
       "         ..., \n",
       "         [ 1240.625,  1018.125,   763.5  , ...,   315.75 ,  -658.5  ,  -777.   ],\n",
       "         [ 1031.75 ,   784.125,   548.   , ...,   258.5  ,  -774.5  ,\n",
       "           -934.625],\n",
       "         [ 1457.5  ,  1217.5  ,   960.875, ...,   461.625,  -602.25 ,\n",
       "           -703.25 ]],\n",
       " \n",
       "        [[  725.   ,   460.375,   208.625, ...,   571.625,  -458.125,\n",
       "           -515.625],\n",
       "         [  620.5  ,   347.125,   123.125, ...,   619.   ,  -266.5  ,\n",
       "           -403.375],\n",
       "         [  499.5  ,   232.   ,    -3.25 , ...,   589.625,  -310.125,\n",
       "           -338.625],\n",
       "         ..., \n",
       "         [ 1392.125,  1172.75 ,   925.875, ...,   162.   ,  -845.75 ,  -995.   ],\n",
       "         [ 1382.75 ,  1177.   ,   926.625, ...,   311.625,  -668.125,\n",
       "           -822.625],\n",
       "         [ 1386.5  ,  1147.875,   899.625, ...,   439.125,  -645.375,\n",
       "           -801.875]],\n",
       " \n",
       "        ..., \n",
       "        [[ 2366.75 ,  2498.75 ,  2328.25 , ...,   605.875,   439.875,   194.   ],\n",
       "         [ 2620.5  ,  2764.875,  2611.125, ...,   597.375,   335.125,\n",
       "             89.125],\n",
       "         [ 2482.   ,  2645.125,  2480.25 , ...,   420.25 ,   281.75 ,\n",
       "             80.5  ],\n",
       "         ..., \n",
       "         [ 2437.125,  2590.125,  2459.875, ...,  1167.625,   679.875,\n",
       "            740.75 ],\n",
       "         [ 2490.125,  2641.   ,  2476.25 , ...,  1002.625,   521.375,\n",
       "            382.25 ],\n",
       "         [ 2329.875,  2467.625,  2338.625, ...,   759.75 ,   452.25 ,\n",
       "            675.625]],\n",
       " \n",
       "        [[ 2544.5  ,  2700.   ,  2513.875, ...,   655.125,   213.5  ,\n",
       "           -133.875],\n",
       "         [ 2279.   ,  2427.375,  2228.5  , ...,   886.375,   278.25 ,\n",
       "           -125.125],\n",
       "         [ 2436.25 ,  2572.125,  2408.   , ...,   362.75 ,    18.5  ,\n",
       "           -153.75 ],\n",
       "         ..., \n",
       "         [ 2455.875,  2627.125,  2440.5  , ...,  1031.   ,   357.125,\n",
       "            182.25 ],\n",
       "         [ 2087.25 ,  2234.125,  2094.5  , ...,   710.875,   114.875,\n",
       "             10.75 ],\n",
       "         [ 2140.   ,  2287.   ,  2152.5  , ...,   896.875,   331.875,\n",
       "            312.25 ]],\n",
       " \n",
       "        [[ 2329.   ,  2470.5  ,  2314.   , ...,   541.125,   289.625,\n",
       "            201.625],\n",
       "         [ 2343.625,  2500.125,  2323.375, ...,   778.875,   424.875,\n",
       "            342.625],\n",
       "         [ 2508.5  ,  2654.75 ,  2496.875, ...,   537.875,   194.625,\n",
       "             70.5  ],\n",
       "         ..., \n",
       "         [ 2068.5  ,  2218.   ,  2066.75 , ...,   486.125,   -34.375,\n",
       "            -71.625],\n",
       "         [ 2019.   ,  2149.75 ,  2021.   , ...,   910.625,   216.625,   244.   ],\n",
       "         [ 2021.   ,  2157.   ,  2035.75 , ...,   553.375,   -10.75 ,\n",
       "            -38.625]]]), array([[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcd_res = readCompleteMatFile(r'C:\\Users\\ORI\\Documents\\Thesis\\dataset_all\\{0}'.format(\"RSVP_Color116msVPicr.mat\"))\n",
    "gcd_res.keys()\n",
    "to_categorical(gcd_res['stimulus'])\n",
    "create_train_data(gcd_res,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_compile_lstm_model_letter()\n",
    "# model_mlp = create_small_compile_dense_model()\n",
    "original_weights = model.get_weights()\n",
    "# original_weights_mlp = model_mlp.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18600L, 200L, 55L)\n",
      "len(categorical_tags) 13200\n",
      "(13200L, 31L)\n",
      "(18600L, 200L, 55L)\n",
      "len(categorical_tags) 5400\n",
      "Train on 13200 samples, validate on 5400 samples\n",
      "Epoch 1/400\n",
      "13200/13200 [==============================] - 33s - loss: 3.4424 - acc: 0.0345 - val_loss: 3.4206 - val_acc: 0.0320\n",
      "Epoch 2/400\n",
      "13200/13200 [==============================] - 32s - loss: 3.4212 - acc: 0.0359 - val_loss: 3.4081 - val_acc: 0.0350\n",
      "Epoch 3/400\n",
      "13200/13200 [==============================] - 32s - loss: 3.4076 - acc: 0.0427 - val_loss: 3.3970 - val_acc: 0.0385\n",
      "Epoch 4/400\n",
      "13200/13200 [==============================] - 33s - loss: 3.3955 - acc: 0.0468 - val_loss: 3.3906 - val_acc: 0.0367\n",
      "Epoch 5/400\n",
      "13200/13200 [==============================] - 32s - loss: 3.3851 - acc: 0.0504 - val_loss: 3.3850 - val_acc: 0.0415\n",
      "Epoch 6/400\n",
      "13200/13200 [==============================] - 32s - loss: 3.3729 - acc: 0.0533 - val_loss: 3.3809 - val_acc: 0.0441\n",
      "Epoch 7/400\n",
      "13200/13200 [==============================] - 32s - loss: 3.3575 - acc: 0.0598 - val_loss: 3.3663 - val_acc: 0.0443\n",
      "Epoch 8/400\n",
      "13200/13200 [==============================] - 32s - loss: 3.3355 - acc: 0.0651 - val_loss: 3.3480 - val_acc: 0.0491\n",
      "Epoch 9/400\n",
      "13200/13200 [==============================] - 33s - loss: 3.3019 - acc: 0.0723 - val_loss: 3.3218 - val_acc: 0.0519\n",
      "Epoch 10/400\n",
      "13200/13200 [==============================] - 33s - loss: 3.2615 - acc: 0.0752 - val_loss: 3.2995 - val_acc: 0.0543\n",
      "Epoch 11/400\n",
      "13200/13200 [==============================] - 33s - loss: 3.2208 - acc: 0.0844 - val_loss: 3.2833 - val_acc: 0.0589\n",
      "Epoch 12/400\n",
      "13200/13200 [==============================] - 33s - loss: 3.1779 - acc: 0.0877 - val_loss: 3.2537 - val_acc: 0.0637\n",
      "Epoch 13/400\n",
      "13200/13200 [==============================] - 32s - loss: 3.1348 - acc: 0.0927 - val_loss: 3.2202 - val_acc: 0.0700\n",
      "Epoch 14/400\n",
      "13200/13200 [==============================] - 34s - loss: 3.0939 - acc: 0.0977 - val_loss: 3.2104 - val_acc: 0.0683\n",
      "Epoch 15/400\n",
      "13200/13200 [==============================] - 37s - loss: 3.0544 - acc: 0.1020 - val_loss: 3.1937 - val_acc: 0.0772\n",
      "Epoch 16/400\n",
      "13200/13200 [==============================] - 35s - loss: 3.0175 - acc: 0.1055 - val_loss: 3.2037 - val_acc: 0.0681\n",
      "Epoch 17/400\n",
      "13200/13200 [==============================] - 32s - loss: 2.9800 - acc: 0.1070 - val_loss: 3.1810 - val_acc: 0.0735\n",
      "Epoch 18/400\n",
      "13200/13200 [==============================] - 39s - loss: 2.9439 - acc: 0.1132 - val_loss: 3.1759 - val_acc: 0.0757\n",
      "Epoch 19/400\n",
      "13200/13200 [==============================] - 50s - loss: 2.9128 - acc: 0.1145 - val_loss: 3.1695 - val_acc: 0.0774\n",
      "Epoch 20/400\n",
      "13200/13200 [==============================] - 34s - loss: 2.8809 - acc: 0.1180 - val_loss: 3.1755 - val_acc: 0.0789\n",
      "Epoch 21/400\n",
      "13200/13200 [==============================] - 34s - loss: 2.8532 - acc: 0.1253 - val_loss: 3.1823 - val_acc: 0.0802\n",
      "Epoch 22/400\n",
      "13200/13200 [==============================] - 34s - loss: 2.8253 - acc: 0.1227 - val_loss: 3.1760 - val_acc: 0.0831\n",
      "Epoch 23/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.7962 - acc: 0.1304 - val_loss: 3.1665 - val_acc: 0.0813\n",
      "Epoch 24/400\n",
      "13200/13200 [==============================] - 32s - loss: 2.7686 - acc: 0.1286 - val_loss: 3.1364 - val_acc: 0.0889\n",
      "Epoch 25/400\n",
      "13200/13200 [==============================] - 32s - loss: 2.7372 - acc: 0.1345 - val_loss: 3.1544 - val_acc: 0.0891\n",
      "Epoch 26/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.7125 - acc: 0.1408 - val_loss: 3.1228 - val_acc: 0.0874\n",
      "Epoch 27/400\n",
      "13200/13200 [==============================] - 32s - loss: 2.6829 - acc: 0.1436 - val_loss: 3.1429 - val_acc: 0.0863\n",
      "Epoch 28/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.6589 - acc: 0.1493 - val_loss: 3.1123 - val_acc: 0.0954\n",
      "Epoch 29/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.6262 - acc: 0.1523 - val_loss: 3.1426 - val_acc: 0.0930\n",
      "Epoch 30/400\n",
      "13200/13200 [==============================] - 43s - loss: 2.6042 - acc: 0.1515 - val_loss: 3.1243 - val_acc: 0.0957\n",
      "Epoch 31/400\n",
      "13200/13200 [==============================] - 46s - loss: 2.5801 - acc: 0.1552 - val_loss: 3.1544 - val_acc: 0.0952\n",
      "Epoch 32/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.5532 - acc: 0.1583 - val_loss: 3.0896 - val_acc: 0.1059\n",
      "Epoch 33/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.5277 - acc: 0.1684 - val_loss: 3.1313 - val_acc: 0.1030\n",
      "Epoch 34/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.5075 - acc: 0.1671 - val_loss: 3.1781 - val_acc: 0.0967\n",
      "Epoch 35/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.4847 - acc: 0.1688 - val_loss: 3.0734 - val_acc: 0.1056\n",
      "Epoch 36/400\n",
      "13200/13200 [==============================] - 34s - loss: 2.4589 - acc: 0.1692 - val_loss: 3.0979 - val_acc: 0.1074\n",
      "Epoch 37/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.4366 - acc: 0.1714 - val_loss: 3.0916 - val_acc: 0.0996\n",
      "Epoch 38/400\n",
      "13200/13200 [==============================] - 40s - loss: 2.4120 - acc: 0.1752 - val_loss: 3.1871 - val_acc: 0.1065\n",
      "Epoch 39/400\n",
      "13200/13200 [==============================] - 44s - loss: 2.3904 - acc: 0.1823 - val_loss: 3.1049 - val_acc: 0.1059\n",
      "Epoch 40/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.3681 - acc: 0.1789 - val_loss: 3.0913 - val_acc: 0.1070\n",
      "Epoch 41/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.3482 - acc: 0.1849 - val_loss: 3.0908 - val_acc: 0.1111\n",
      "Epoch 42/400\n",
      "13200/13200 [==============================] - 34s - loss: 2.3292 - acc: 0.1843 - val_loss: 3.0474 - val_acc: 0.1150\n",
      "Epoch 43/400\n",
      "13200/13200 [==============================] - 50s - loss: 2.3123 - acc: 0.1847 - val_loss: 3.0442 - val_acc: 0.1187\n",
      "Epoch 44/400\n",
      "13200/13200 [==============================] - 40s - loss: 2.2941 - acc: 0.1896 - val_loss: 3.0438 - val_acc: 0.1126\n",
      "Epoch 45/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.2778 - acc: 0.1927 - val_loss: 3.1184 - val_acc: 0.1128\n",
      "Epoch 46/400\n",
      "13200/13200 [==============================] - 32s - loss: 2.2615 - acc: 0.1898 - val_loss: 3.0824 - val_acc: 0.1169\n",
      "Epoch 47/400\n",
      "13200/13200 [==============================] - 32s - loss: 2.2447 - acc: 0.1926 - val_loss: 3.1072 - val_acc: 0.1183\n",
      "Epoch 48/400\n",
      "13200/13200 [==============================] - 32s - loss: 2.2309 - acc: 0.1926 - val_loss: 3.0385 - val_acc: 0.1235\n",
      "Epoch 49/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.2124 - acc: 0.1955 - val_loss: 3.0891 - val_acc: 0.1120\n",
      "Epoch 50/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.2008 - acc: 0.1969 - val_loss: 3.0680 - val_acc: 0.1196\n",
      "Epoch 51/400\n",
      "13200/13200 [==============================] - 33s - loss: 2.1862 - acc: 0.1955 - val_loss: 3.0556 - val_acc: 0.1217\n",
      "Epoch 52/400\n",
      "13200/13200 [==============================] - 36s - loss: 2.1726 - acc: 0.1949 - val_loss: 3.0524 - val_acc: 0.1313\n",
      "Epoch 53/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.1621 - acc: 0.2011 - val_loss: 3.0924 - val_acc: 0.1220\n",
      "Epoch 54/400\n",
      "13200/13200 [==============================] - 34s - loss: 2.1482 - acc: 0.2047 - val_loss: 3.0143 - val_acc: 0.1302\n",
      "Epoch 55/400\n",
      "13200/13200 [==============================] - 34s - loss: 2.1363 - acc: 0.2003 - val_loss: 2.9947 - val_acc: 0.1281\n",
      "Epoch 56/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.1239 - acc: 0.2032 - val_loss: 3.0493 - val_acc: 0.1250\n",
      "Epoch 57/400\n",
      "13200/13200 [==============================] - 46s - loss: 2.1157 - acc: 0.2008 - val_loss: 3.0390 - val_acc: 0.1337\n",
      "Epoch 58/400\n",
      "13200/13200 [==============================] - 46s - loss: 2.1046 - acc: 0.2064 - val_loss: 3.0500 - val_acc: 0.1298\n",
      "Epoch 59/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0936 - acc: 0.2034 - val_loss: 3.0161 - val_acc: 0.1285\n",
      "Epoch 60/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0881 - acc: 0.2090 - val_loss: 3.0244 - val_acc: 0.1287\n",
      "Epoch 61/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0776 - acc: 0.2048 - val_loss: 3.0592 - val_acc: 0.1293\n",
      "Epoch 62/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0679 - acc: 0.2107 - val_loss: 2.9860 - val_acc: 0.1304\n",
      "Epoch 63/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0605 - acc: 0.2109 - val_loss: 3.0401 - val_acc: 0.1248\n",
      "Epoch 64/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0538 - acc: 0.2113 - val_loss: 3.0521 - val_acc: 0.1289\n",
      "Epoch 65/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0461 - acc: 0.2088 - val_loss: 3.0219 - val_acc: 0.1333\n",
      "Epoch 66/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0399 - acc: 0.2118 - val_loss: 3.0078 - val_acc: 0.1278\n",
      "Epoch 67/400\n",
      "13200/13200 [==============================] - 36s - loss: 2.0354 - acc: 0.2148 - val_loss: 3.0088 - val_acc: 0.1224\n",
      "Epoch 68/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0236 - acc: 0.2189 - val_loss: 3.0599 - val_acc: 0.1331\n",
      "Epoch 69/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0199 - acc: 0.2145 - val_loss: 3.0074 - val_acc: 0.1285\n",
      "Epoch 70/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0121 - acc: 0.2198 - val_loss: 3.0271 - val_acc: 0.1389\n",
      "Epoch 71/400\n",
      "13200/13200 [==============================] - 36s - loss: 2.0084 - acc: 0.2171 - val_loss: 3.0841 - val_acc: 0.1341\n",
      "Epoch 72/400\n",
      "13200/13200 [==============================] - 35s - loss: 2.0013 - acc: 0.2178 - val_loss: 3.0283 - val_acc: 0.1256\n",
      "Epoch 73/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9960 - acc: 0.2142 - val_loss: 2.9863 - val_acc: 0.1363\n",
      "Epoch 74/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9929 - acc: 0.2195 - val_loss: 3.0218 - val_acc: 0.1369\n",
      "Epoch 75/400\n",
      "13200/13200 [==============================] - 36s - loss: 1.9871 - acc: 0.2220 - val_loss: 3.0297 - val_acc: 0.1324\n",
      "Epoch 76/400\n",
      "13200/13200 [==============================] - 36s - loss: 1.9786 - acc: 0.2228 - val_loss: 3.0116 - val_acc: 0.1348\n",
      "Epoch 77/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9803 - acc: 0.2169 - val_loss: 3.0209 - val_acc: 0.1378\n",
      "Epoch 78/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9710 - acc: 0.2197 - val_loss: 2.9830 - val_acc: 0.1307\n",
      "Epoch 79/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9703 - acc: 0.2258 - val_loss: 2.9987 - val_acc: 0.1419\n",
      "Epoch 80/400\n",
      "13200/13200 [==============================] - 36s - loss: 1.9619 - acc: 0.2282 - val_loss: 2.9498 - val_acc: 0.1406\n",
      "Epoch 81/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9613 - acc: 0.2273 - val_loss: 2.9365 - val_acc: 0.1394\n",
      "Epoch 82/400\n",
      "13200/13200 [==============================] - 36s - loss: 1.9562 - acc: 0.2284 - val_loss: 2.9656 - val_acc: 0.1400\n",
      "Epoch 83/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9496 - acc: 0.2305 - val_loss: 2.9653 - val_acc: 0.1407\n",
      "Epoch 84/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9501 - acc: 0.2267 - val_loss: 2.9736 - val_acc: 0.1361\n",
      "Epoch 85/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9412 - acc: 0.2302 - val_loss: 2.9621 - val_acc: 0.1369\n",
      "Epoch 86/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9433 - acc: 0.2309 - val_loss: 2.9790 - val_acc: 0.1315\n",
      "Epoch 87/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9359 - acc: 0.2336 - val_loss: 2.9794 - val_acc: 0.1343\n",
      "Epoch 88/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9338 - acc: 0.2317 - val_loss: 2.9063 - val_acc: 0.1352\n",
      "Epoch 89/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9314 - acc: 0.2352 - val_loss: 2.9981 - val_acc: 0.1389\n",
      "Epoch 90/400\n",
      "13200/13200 [==============================] - 36s - loss: 1.9281 - acc: 0.2325 - val_loss: 2.9422 - val_acc: 0.1383\n",
      "Epoch 91/400\n",
      "13200/13200 [==============================] - 36s - loss: 1.9229 - acc: 0.2369 - val_loss: 2.9370 - val_acc: 0.1363\n",
      "Epoch 92/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9169 - acc: 0.2355 - val_loss: 2.9840 - val_acc: 0.1396\n",
      "Epoch 93/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9188 - acc: 0.2341 - val_loss: 2.9753 - val_acc: 0.1424\n",
      "Epoch 94/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9115 - acc: 0.2408 - val_loss: 2.9853 - val_acc: 0.1450\n",
      "Epoch 95/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9126 - acc: 0.2409 - val_loss: 2.9461 - val_acc: 0.1461\n",
      "Epoch 96/400\n",
      "13200/13200 [==============================] - 36s - loss: 1.9089 - acc: 0.2361 - val_loss: 2.9259 - val_acc: 0.1391\n",
      "Epoch 97/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9072 - acc: 0.2395 - val_loss: 3.0261 - val_acc: 0.1372\n",
      "Epoch 98/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.9039 - acc: 0.2456 - val_loss: 2.9789 - val_acc: 0.1452\n",
      "Epoch 99/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8996 - acc: 0.2430 - val_loss: 2.9456 - val_acc: 0.1441\n",
      "Epoch 100/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8951 - acc: 0.2435 - val_loss: 2.9695 - val_acc: 0.1409\n",
      "Epoch 101/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8918 - acc: 0.2426 - val_loss: 2.9606 - val_acc: 0.1396\n",
      "Epoch 102/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8913 - acc: 0.2456 - val_loss: 2.9913 - val_acc: 0.1450\n",
      "Epoch 103/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8880 - acc: 0.2457 - val_loss: 2.9428 - val_acc: 0.1411\n",
      "Epoch 104/400\n",
      "13200/13200 [==============================] - 36s - loss: 1.8853 - acc: 0.2491 - val_loss: 2.9267 - val_acc: 0.1441\n",
      "Epoch 105/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8817 - acc: 0.2542 - val_loss: 2.9844 - val_acc: 0.1389\n",
      "Epoch 106/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8796 - acc: 0.2528 - val_loss: 2.9448 - val_acc: 0.1444\n",
      "Epoch 107/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8779 - acc: 0.2489 - val_loss: 2.9629 - val_acc: 0.1463\n",
      "Epoch 108/400\n",
      "13200/13200 [==============================] - 35s - loss: 1.8730 - acc: 0.2504 - val_loss: 2.9336 - val_acc: 0.1469\n",
      "Epoch 109/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8731 - acc: 0.2533 - val_loss: 2.9648 - val_acc: 0.1463\n",
      "Epoch 110/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8706 - acc: 0.2523 - val_loss: 2.9970 - val_acc: 0.1356\n",
      "Epoch 111/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8681 - acc: 0.2561 - val_loss: 2.9352 - val_acc: 0.1391\n",
      "Epoch 112/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8654 - acc: 0.2591 - val_loss: 3.0035 - val_acc: 0.1400\n",
      "Epoch 113/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8645 - acc: 0.2545 - val_loss: 2.9631 - val_acc: 0.1491\n",
      "Epoch 114/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8590 - acc: 0.2623 - val_loss: 2.9915 - val_acc: 0.1441\n",
      "Epoch 115/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8577 - acc: 0.2577 - val_loss: 2.9247 - val_acc: 0.1419\n",
      "Epoch 116/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8540 - acc: 0.2555 - val_loss: 2.9860 - val_acc: 0.1454\n",
      "Epoch 117/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8533 - acc: 0.2666 - val_loss: 2.9381 - val_acc: 0.1385\n",
      "Epoch 118/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8508 - acc: 0.2613 - val_loss: 2.9325 - val_acc: 0.1441\n",
      "Epoch 119/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8513 - acc: 0.2633 - val_loss: 2.9197 - val_acc: 0.1439\n",
      "Epoch 120/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8468 - acc: 0.2646 - val_loss: 2.9174 - val_acc: 0.1480\n",
      "Epoch 121/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8444 - acc: 0.2661 - val_loss: 2.9310 - val_acc: 0.1448\n",
      "Epoch 122/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8437 - acc: 0.2685 - val_loss: 2.9104 - val_acc: 0.1520\n",
      "Epoch 123/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8375 - acc: 0.2686 - val_loss: 2.9581 - val_acc: 0.1459\n",
      "Epoch 124/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8402 - acc: 0.2670 - val_loss: 2.9193 - val_acc: 0.1459\n",
      "Epoch 125/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8342 - acc: 0.2734 - val_loss: 2.9967 - val_acc: 0.1448\n",
      "Epoch 126/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8308 - acc: 0.2729 - val_loss: 2.9695 - val_acc: 0.1481\n",
      "Epoch 127/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8298 - acc: 0.2755 - val_loss: 2.9869 - val_acc: 0.1463\n",
      "Epoch 128/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8286 - acc: 0.2730 - val_loss: 2.9507 - val_acc: 0.1446\n",
      "Epoch 129/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8268 - acc: 0.2764 - val_loss: 2.9469 - val_acc: 0.1431\n",
      "Epoch 130/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8219 - acc: 0.2764 - val_loss: 2.9458 - val_acc: 0.1502\n",
      "Epoch 131/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8229 - acc: 0.2806 - val_loss: 2.9788 - val_acc: 0.1478\n",
      "Epoch 132/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8216 - acc: 0.2755 - val_loss: 2.9433 - val_acc: 0.1391\n",
      "Epoch 133/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8171 - acc: 0.2807 - val_loss: 2.9589 - val_acc: 0.1426\n",
      "Epoch 134/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8152 - acc: 0.2790 - val_loss: 2.9804 - val_acc: 0.1426\n",
      "Epoch 135/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8149 - acc: 0.2808 - val_loss: 2.9636 - val_acc: 0.1489\n",
      "Epoch 136/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8150 - acc: 0.2817 - val_loss: 2.9535 - val_acc: 0.1444\n",
      "Epoch 137/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8106 - acc: 0.2849 - val_loss: 2.9752 - val_acc: 0.1452\n",
      "Epoch 138/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8057 - acc: 0.2860 - val_loss: 2.9387 - val_acc: 0.1520\n",
      "Epoch 139/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8076 - acc: 0.2862 - val_loss: 2.9761 - val_acc: 0.1452\n",
      "Epoch 140/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8040 - acc: 0.2884 - val_loss: 2.9354 - val_acc: 0.1457\n",
      "Epoch 141/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7994 - acc: 0.2933 - val_loss: 2.9438 - val_acc: 0.1459\n",
      "Epoch 142/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.8014 - acc: 0.2935 - val_loss: 2.9606 - val_acc: 0.1496\n",
      "Epoch 143/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7952 - acc: 0.2890 - val_loss: 2.9841 - val_acc: 0.1444\n",
      "Epoch 144/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7959 - acc: 0.2915 - val_loss: 2.9545 - val_acc: 0.1465\n",
      "Epoch 145/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7921 - acc: 0.2925 - val_loss: 2.9740 - val_acc: 0.1472\n",
      "Epoch 146/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7901 - acc: 0.2934 - val_loss: 2.9805 - val_acc: 0.1454\n",
      "Epoch 147/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7935 - acc: 0.2896 - val_loss: 3.0214 - val_acc: 0.1394\n",
      "Epoch 148/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7878 - acc: 0.2937 - val_loss: 2.9648 - val_acc: 0.1444\n",
      "Epoch 149/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7849 - acc: 0.2978 - val_loss: 3.0129 - val_acc: 0.1469\n",
      "Epoch 150/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7861 - acc: 0.2987 - val_loss: 2.9681 - val_acc: 0.1469\n",
      "Epoch 151/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7822 - acc: 0.2993 - val_loss: 2.9702 - val_acc: 0.1422\n",
      "Epoch 152/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7825 - acc: 0.3042 - val_loss: 2.9525 - val_acc: 0.1493\n",
      "Epoch 153/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7767 - acc: 0.3044 - val_loss: 2.9956 - val_acc: 0.1450\n",
      "Epoch 154/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7763 - acc: 0.3018 - val_loss: 3.0125 - val_acc: 0.1430\n",
      "Epoch 155/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7765 - acc: 0.3024 - val_loss: 3.0056 - val_acc: 0.1546\n",
      "Epoch 156/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7724 - acc: 0.3035 - val_loss: 2.9748 - val_acc: 0.1500\n",
      "Epoch 157/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7661 - acc: 0.3073 - val_loss: 2.9697 - val_acc: 0.1517\n",
      "Epoch 158/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7691 - acc: 0.3039 - val_loss: 3.0094 - val_acc: 0.1493\n",
      "Epoch 159/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7677 - acc: 0.3064 - val_loss: 2.9953 - val_acc: 0.1420\n",
      "Epoch 160/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7663 - acc: 0.3082 - val_loss: 2.9617 - val_acc: 0.1489\n",
      "Epoch 161/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7625 - acc: 0.3082 - val_loss: 3.0056 - val_acc: 0.1580\n",
      "Epoch 162/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7620 - acc: 0.3114 - val_loss: 2.9713 - val_acc: 0.1535\n",
      "Epoch 163/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7595 - acc: 0.3141 - val_loss: 2.9808 - val_acc: 0.1496\n",
      "Epoch 164/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7557 - acc: 0.3134 - val_loss: 3.0067 - val_acc: 0.1469\n",
      "Epoch 165/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7519 - acc: 0.3132 - val_loss: 2.9756 - val_acc: 0.1496\n",
      "Epoch 166/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.7561 - acc: 0.3158 - val_loss: 2.9617 - val_acc: 0.1459\n",
      "Epoch 167/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7524 - acc: 0.3183 - val_loss: 2.9999 - val_acc: 0.1494\n",
      "Epoch 168/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7456 - acc: 0.3158 - val_loss: 2.9920 - val_acc: 0.1478\n",
      "Epoch 169/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7454 - acc: 0.3194 - val_loss: 3.0284 - val_acc: 0.1472\n",
      "Epoch 170/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7450 - acc: 0.3161 - val_loss: 3.0147 - val_acc: 0.1528\n",
      "Epoch 171/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7451 - acc: 0.3194 - val_loss: 3.0168 - val_acc: 0.1491\n",
      "Epoch 172/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.7392 - acc: 0.3244 - val_loss: 3.0096 - val_acc: 0.1469\n",
      "Epoch 173/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7391 - acc: 0.3193 - val_loss: 3.0141 - val_acc: 0.1515\n",
      "Epoch 174/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7365 - acc: 0.3257 - val_loss: 3.0278 - val_acc: 0.1487\n",
      "Epoch 175/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7381 - acc: 0.3234 - val_loss: 3.0084 - val_acc: 0.1444\n",
      "Epoch 176/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7358 - acc: 0.3277 - val_loss: 3.0597 - val_acc: 0.1413\n",
      "Epoch 177/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7354 - acc: 0.3275 - val_loss: 3.0149 - val_acc: 0.1485\n",
      "Epoch 178/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7314 - acc: 0.3297 - val_loss: 3.0136 - val_acc: 0.1481\n",
      "Epoch 179/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7287 - acc: 0.3310 - val_loss: 3.0038 - val_acc: 0.1448\n",
      "Epoch 180/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7256 - acc: 0.3293 - val_loss: 3.0235 - val_acc: 0.1480\n",
      "Epoch 181/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7270 - acc: 0.3249 - val_loss: 3.0093 - val_acc: 0.1481\n",
      "Epoch 182/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7244 - acc: 0.3305 - val_loss: 3.0343 - val_acc: 0.1472\n",
      "Epoch 183/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7208 - acc: 0.3349 - val_loss: 3.0542 - val_acc: 0.1420\n",
      "Epoch 184/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7223 - acc: 0.3342 - val_loss: 3.0254 - val_acc: 0.1470\n",
      "Epoch 185/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7147 - acc: 0.3373 - val_loss: 3.0160 - val_acc: 0.1487\n",
      "Epoch 186/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7191 - acc: 0.3390 - val_loss: 3.0517 - val_acc: 0.1450\n",
      "Epoch 187/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7175 - acc: 0.3342 - val_loss: 3.0152 - val_acc: 0.1450\n",
      "Epoch 188/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7149 - acc: 0.3418 - val_loss: 3.0645 - val_acc: 0.1459\n",
      "Epoch 189/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7136 - acc: 0.3342 - val_loss: 3.0565 - val_acc: 0.1491\n",
      "Epoch 190/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7073 - acc: 0.3408 - val_loss: 3.0669 - val_acc: 0.1441\n",
      "Epoch 191/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7083 - acc: 0.3415 - val_loss: 3.0365 - val_acc: 0.1456\n",
      "Epoch 192/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.7037 - acc: 0.3423 - val_loss: 3.0090 - val_acc: 0.1448\n",
      "Epoch 193/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7079 - acc: 0.3399 - val_loss: 3.0282 - val_acc: 0.1474\n",
      "Epoch 194/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7047 - acc: 0.3448 - val_loss: 3.0652 - val_acc: 0.1419\n",
      "Epoch 195/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.7054 - acc: 0.3432 - val_loss: 3.0254 - val_acc: 0.1500\n",
      "Epoch 196/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6968 - acc: 0.3485 - val_loss: 3.0327 - val_acc: 0.1504\n",
      "Epoch 197/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6979 - acc: 0.3467 - val_loss: 3.0813 - val_acc: 0.1448\n",
      "Epoch 198/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6978 - acc: 0.3465 - val_loss: 3.0769 - val_acc: 0.1441\n",
      "Epoch 199/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6946 - acc: 0.3484 - val_loss: 3.0164 - val_acc: 0.1507\n",
      "Epoch 200/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6938 - acc: 0.3491 - val_loss: 3.0967 - val_acc: 0.1452\n",
      "Epoch 201/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6912 - acc: 0.3476 - val_loss: 3.0573 - val_acc: 0.1504\n",
      "Epoch 202/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6931 - acc: 0.3517 - val_loss: 3.0653 - val_acc: 0.1530\n",
      "Epoch 203/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6861 - acc: 0.3520 - val_loss: 3.0864 - val_acc: 0.1496\n",
      "Epoch 204/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6835 - acc: 0.3540 - val_loss: 3.0779 - val_acc: 0.1444\n",
      "Epoch 205/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6859 - acc: 0.3527 - val_loss: 3.0721 - val_acc: 0.1483\n",
      "Epoch 206/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6833 - acc: 0.3526 - val_loss: 3.0461 - val_acc: 0.1511\n",
      "Epoch 207/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6814 - acc: 0.3611 - val_loss: 3.0674 - val_acc: 0.1554\n",
      "Epoch 208/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6780 - acc: 0.3582 - val_loss: 3.0834 - val_acc: 0.1441\n",
      "Epoch 209/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6796 - acc: 0.3561 - val_loss: 3.1142 - val_acc: 0.1441\n",
      "Epoch 210/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6786 - acc: 0.3539 - val_loss: 3.0845 - val_acc: 0.1515\n",
      "Epoch 211/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6722 - acc: 0.3617 - val_loss: 3.1004 - val_acc: 0.1411\n",
      "Epoch 212/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6748 - acc: 0.3579 - val_loss: 3.0919 - val_acc: 0.1474\n",
      "Epoch 213/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6688 - acc: 0.3619 - val_loss: 3.0864 - val_acc: 0.1419\n",
      "Epoch 214/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6781 - acc: 0.3570 - val_loss: 3.0894 - val_acc: 0.1519\n",
      "Epoch 215/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6677 - acc: 0.3661 - val_loss: 3.0945 - val_acc: 0.1452\n",
      "Epoch 216/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6713 - acc: 0.3657 - val_loss: 3.1175 - val_acc: 0.1483\n",
      "Epoch 217/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6649 - acc: 0.3633 - val_loss: 3.0764 - val_acc: 0.1478\n",
      "Epoch 218/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.6672 - acc: 0.3670 - val_loss: 3.1190 - val_acc: 0.1437\n",
      "Epoch 219/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6635 - acc: 0.3666 - val_loss: 3.0733 - val_acc: 0.1465\n",
      "Epoch 220/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6607 - acc: 0.3658 - val_loss: 3.1333 - val_acc: 0.1439\n",
      "Epoch 221/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6594 - acc: 0.3661 - val_loss: 3.1286 - val_acc: 0.1452\n",
      "Epoch 222/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6590 - acc: 0.3714 - val_loss: 3.0921 - val_acc: 0.1511\n",
      "Epoch 223/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6582 - acc: 0.3670 - val_loss: 3.0532 - val_acc: 0.1467\n",
      "Epoch 224/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6550 - acc: 0.3711 - val_loss: 3.1145 - val_acc: 0.1394\n",
      "Epoch 225/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6533 - acc: 0.3705 - val_loss: 3.1375 - val_acc: 0.1411\n",
      "Epoch 226/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6492 - acc: 0.3778 - val_loss: 3.0836 - val_acc: 0.1457\n",
      "Epoch 227/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6486 - acc: 0.3798 - val_loss: 3.1249 - val_acc: 0.1476\n",
      "Epoch 228/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6497 - acc: 0.3704 - val_loss: 3.1039 - val_acc: 0.1459\n",
      "Epoch 229/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6481 - acc: 0.3739 - val_loss: 3.1172 - val_acc: 0.1480\n",
      "Epoch 230/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6516 - acc: 0.3733 - val_loss: 3.0901 - val_acc: 0.1502\n",
      "Epoch 231/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6429 - acc: 0.3749 - val_loss: 3.1463 - val_acc: 0.1478\n",
      "Epoch 232/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6460 - acc: 0.3776 - val_loss: 3.1351 - val_acc: 0.1469\n",
      "Epoch 233/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6417 - acc: 0.3777 - val_loss: 3.1178 - val_acc: 0.1465\n",
      "Epoch 234/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6412 - acc: 0.3785 - val_loss: 3.1333 - val_acc: 0.1470\n",
      "Epoch 235/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6411 - acc: 0.3823 - val_loss: 3.1342 - val_acc: 0.1459\n",
      "Epoch 236/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6392 - acc: 0.3817 - val_loss: 3.1843 - val_acc: 0.1435\n",
      "Epoch 237/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6371 - acc: 0.3817 - val_loss: 3.1266 - val_acc: 0.1454\n",
      "Epoch 238/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6370 - acc: 0.3843 - val_loss: 3.1705 - val_acc: 0.1459\n",
      "Epoch 239/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6345 - acc: 0.3827 - val_loss: 3.1566 - val_acc: 0.1430\n",
      "Epoch 240/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6336 - acc: 0.3848 - val_loss: 3.1422 - val_acc: 0.1443\n",
      "Epoch 241/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6278 - acc: 0.3848 - val_loss: 3.1533 - val_acc: 0.1469\n",
      "Epoch 242/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6327 - acc: 0.3877 - val_loss: 3.1254 - val_acc: 0.1467\n",
      "Epoch 243/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.6221 - acc: 0.3909 - val_loss: 3.1467 - val_acc: 0.1443\n",
      "Epoch 244/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6285 - acc: 0.3889 - val_loss: 3.1724 - val_acc: 0.1465\n",
      "Epoch 245/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6301 - acc: 0.3887 - val_loss: 3.1415 - val_acc: 0.1428\n",
      "Epoch 246/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6225 - acc: 0.3881 - val_loss: 3.1993 - val_acc: 0.1443\n",
      "Epoch 247/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6290 - acc: 0.3898 - val_loss: 3.1922 - val_acc: 0.1393\n",
      "Epoch 248/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6195 - acc: 0.3931 - val_loss: 3.1601 - val_acc: 0.1493\n",
      "Epoch 249/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6242 - acc: 0.3936 - val_loss: 3.1513 - val_acc: 0.1472\n",
      "Epoch 250/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6197 - acc: 0.3921 - val_loss: 3.1908 - val_acc: 0.1465\n",
      "Epoch 251/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6147 - acc: 0.3944 - val_loss: 3.1526 - val_acc: 0.1443\n",
      "Epoch 252/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6122 - acc: 0.3953 - val_loss: 3.1792 - val_acc: 0.1454\n",
      "Epoch 253/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6148 - acc: 0.3964 - val_loss: 3.2211 - val_acc: 0.1356\n",
      "Epoch 254/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6134 - acc: 0.3955 - val_loss: 3.1930 - val_acc: 0.1437\n",
      "Epoch 255/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6118 - acc: 0.3909 - val_loss: 3.1872 - val_acc: 0.1437\n",
      "Epoch 256/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6081 - acc: 0.3970 - val_loss: 3.1596 - val_acc: 0.1446\n",
      "Epoch 257/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6115 - acc: 0.3982 - val_loss: 3.1780 - val_acc: 0.1469\n",
      "Epoch 258/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6046 - acc: 0.4001 - val_loss: 3.2099 - val_acc: 0.1430\n",
      "Epoch 259/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6089 - acc: 0.4014 - val_loss: 3.1717 - val_acc: 0.1450\n",
      "Epoch 260/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6056 - acc: 0.3985 - val_loss: 3.1719 - val_acc: 0.1413\n",
      "Epoch 261/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6094 - acc: 0.4006 - val_loss: 3.2011 - val_acc: 0.1470\n",
      "Epoch 262/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6041 - acc: 0.4020 - val_loss: 3.1703 - val_acc: 0.1454\n",
      "Epoch 263/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5975 - acc: 0.4019 - val_loss: 3.1774 - val_acc: 0.1465\n",
      "Epoch 264/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6027 - acc: 0.4020 - val_loss: 3.1900 - val_acc: 0.1448\n",
      "Epoch 265/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.6004 - acc: 0.4058 - val_loss: 3.1819 - val_acc: 0.1422\n",
      "Epoch 266/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5933 - acc: 0.4039 - val_loss: 3.1906 - val_acc: 0.1456\n",
      "Epoch 267/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5989 - acc: 0.4070 - val_loss: 3.1546 - val_acc: 0.1419\n",
      "Epoch 268/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5957 - acc: 0.4035 - val_loss: 3.2272 - val_acc: 0.1407\n",
      "Epoch 269/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.5903 - acc: 0.4097 - val_loss: 3.1783 - val_acc: 0.1472\n",
      "Epoch 270/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5978 - acc: 0.4073 - val_loss: 3.1882 - val_acc: 0.1472\n",
      "Epoch 271/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5897 - acc: 0.4064 - val_loss: 3.2143 - val_acc: 0.1467\n",
      "Epoch 272/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5916 - acc: 0.4061 - val_loss: 3.1967 - val_acc: 0.1483\n",
      "Epoch 273/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5886 - acc: 0.4143 - val_loss: 3.2166 - val_acc: 0.1439\n",
      "Epoch 274/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5876 - acc: 0.4118 - val_loss: 3.2008 - val_acc: 0.1500\n",
      "Epoch 275/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5887 - acc: 0.4087 - val_loss: 3.1981 - val_acc: 0.1472\n",
      "Epoch 276/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5833 - acc: 0.4114 - val_loss: 3.2240 - val_acc: 0.1391\n",
      "Epoch 277/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5843 - acc: 0.4136 - val_loss: 3.2314 - val_acc: 0.1393\n",
      "Epoch 278/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5858 - acc: 0.4117 - val_loss: 3.2153 - val_acc: 0.1467\n",
      "Epoch 279/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5875 - acc: 0.4161 - val_loss: 3.2222 - val_acc: 0.1469\n",
      "Epoch 280/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5851 - acc: 0.4148 - val_loss: 3.2101 - val_acc: 0.1385\n",
      "Epoch 281/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5806 - acc: 0.4180 - val_loss: 3.2613 - val_acc: 0.1409\n",
      "Epoch 282/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5740 - acc: 0.4154 - val_loss: 3.2475 - val_acc: 0.1463\n",
      "Epoch 283/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5778 - acc: 0.4177 - val_loss: 3.2273 - val_acc: 0.1457\n",
      "Epoch 284/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5756 - acc: 0.4189 - val_loss: 3.2290 - val_acc: 0.1452\n",
      "Epoch 285/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5745 - acc: 0.4186 - val_loss: 3.2440 - val_acc: 0.1424\n",
      "Epoch 286/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5755 - acc: 0.4167 - val_loss: 3.2301 - val_acc: 0.1420\n",
      "Epoch 287/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5772 - acc: 0.4164 - val_loss: 3.2598 - val_acc: 0.1431\n",
      "Epoch 288/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5738 - acc: 0.4189 - val_loss: 3.2159 - val_acc: 0.1446\n",
      "Epoch 289/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5695 - acc: 0.4214 - val_loss: 3.2172 - val_acc: 0.1383\n",
      "Epoch 290/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5701 - acc: 0.4197 - val_loss: 3.2417 - val_acc: 0.1444\n",
      "Epoch 291/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5655 - acc: 0.4222 - val_loss: 3.2759 - val_acc: 0.1381\n",
      "Epoch 292/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.5674 - acc: 0.4255 - val_loss: 3.2424 - val_acc: 0.1387\n",
      "Epoch 293/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5656 - acc: 0.4285 - val_loss: 3.2280 - val_acc: 0.1391\n",
      "Epoch 294/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5640 - acc: 0.4195 - val_loss: 3.2731 - val_acc: 0.1437\n",
      "Epoch 295/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.5596 - acc: 0.4247 - val_loss: 3.2369 - val_acc: 0.1404\n",
      "Epoch 296/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5584 - acc: 0.4271 - val_loss: 3.2823 - val_acc: 0.1394\n",
      "Epoch 297/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5615 - acc: 0.4267 - val_loss: 3.2589 - val_acc: 0.1419\n",
      "Epoch 298/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5551 - acc: 0.4295 - val_loss: 3.2611 - val_acc: 0.1439\n",
      "Epoch 299/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5565 - acc: 0.4297 - val_loss: 3.2675 - val_acc: 0.1450\n",
      "Epoch 300/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5587 - acc: 0.4271 - val_loss: 3.2967 - val_acc: 0.1448\n",
      "Epoch 301/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5559 - acc: 0.4289 - val_loss: 3.2611 - val_acc: 0.1450\n",
      "Epoch 302/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5547 - acc: 0.4286 - val_loss: 3.2579 - val_acc: 0.1402\n",
      "Epoch 303/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5518 - acc: 0.4345 - val_loss: 3.2424 - val_acc: 0.1441\n",
      "Epoch 304/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5575 - acc: 0.4311 - val_loss: 3.2414 - val_acc: 0.1476\n",
      "Epoch 305/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5556 - acc: 0.4295 - val_loss: 3.2601 - val_acc: 0.1463\n",
      "Epoch 306/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5469 - acc: 0.4326 - val_loss: 3.2734 - val_acc: 0.1472\n",
      "Epoch 307/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5478 - acc: 0.4381 - val_loss: 3.2756 - val_acc: 0.1415\n",
      "Epoch 308/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5467 - acc: 0.4384 - val_loss: 3.2981 - val_acc: 0.1457\n",
      "Epoch 309/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5500 - acc: 0.4328 - val_loss: 3.2618 - val_acc: 0.1413\n",
      "Epoch 310/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5479 - acc: 0.4362 - val_loss: 3.2852 - val_acc: 0.1461\n",
      "Epoch 311/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5463 - acc: 0.4357 - val_loss: 3.2592 - val_acc: 0.1469\n",
      "Epoch 312/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5452 - acc: 0.4385 - val_loss: 3.3015 - val_acc: 0.1467\n",
      "Epoch 313/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5428 - acc: 0.4360 - val_loss: 3.2950 - val_acc: 0.1465\n",
      "Epoch 314/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5429 - acc: 0.4379 - val_loss: 3.3376 - val_acc: 0.1417\n",
      "Epoch 315/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5394 - acc: 0.4422 - val_loss: 3.3140 - val_acc: 0.1420\n",
      "Epoch 316/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5365 - acc: 0.4374 - val_loss: 3.2885 - val_acc: 0.1452\n",
      "Epoch 317/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5371 - acc: 0.4421 - val_loss: 3.2924 - val_acc: 0.1443\n",
      "Epoch 318/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5343 - acc: 0.4416 - val_loss: 3.2716 - val_acc: 0.1456\n",
      "Epoch 319/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5361 - acc: 0.4403 - val_loss: 3.2859 - val_acc: 0.1435\n",
      "Epoch 320/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5408 - acc: 0.4434 - val_loss: 3.3075 - val_acc: 0.1498\n",
      "Epoch 321/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.5396 - acc: 0.4369 - val_loss: 3.3103 - val_acc: 0.1439\n",
      "Epoch 322/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5347 - acc: 0.4476 - val_loss: 3.3163 - val_acc: 0.1428\n",
      "Epoch 323/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5312 - acc: 0.4430 - val_loss: 3.2871 - val_acc: 0.1431\n",
      "Epoch 324/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5357 - acc: 0.4424 - val_loss: 3.3123 - val_acc: 0.1393\n",
      "Epoch 325/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5297 - acc: 0.4414 - val_loss: 3.3234 - val_acc: 0.1394\n",
      "Epoch 326/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5266 - acc: 0.4473 - val_loss: 3.3186 - val_acc: 0.1470\n",
      "Epoch 327/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5273 - acc: 0.4471 - val_loss: 3.3292 - val_acc: 0.1400\n",
      "Epoch 328/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5266 - acc: 0.4503 - val_loss: 3.3196 - val_acc: 0.1463\n",
      "Epoch 329/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5250 - acc: 0.4486 - val_loss: 3.2989 - val_acc: 0.1444\n",
      "Epoch 330/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5326 - acc: 0.4466 - val_loss: 3.3100 - val_acc: 0.1424\n",
      "Epoch 331/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5175 - acc: 0.4526 - val_loss: 3.2887 - val_acc: 0.1457\n",
      "Epoch 332/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5264 - acc: 0.4478 - val_loss: 3.2999 - val_acc: 0.1461\n",
      "Epoch 333/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5209 - acc: 0.4511 - val_loss: 3.3209 - val_acc: 0.1465\n",
      "Epoch 334/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5214 - acc: 0.4510 - val_loss: 3.3185 - val_acc: 0.1419\n",
      "Epoch 335/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5239 - acc: 0.4494 - val_loss: 3.3085 - val_acc: 0.1380\n",
      "Epoch 336/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5227 - acc: 0.4499 - val_loss: 3.3586 - val_acc: 0.1426\n",
      "Epoch 337/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5182 - acc: 0.4523 - val_loss: 3.3198 - val_acc: 0.1435\n",
      "Epoch 338/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5177 - acc: 0.4583 - val_loss: 3.3431 - val_acc: 0.1457\n",
      "Epoch 339/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5223 - acc: 0.4532 - val_loss: 3.3002 - val_acc: 0.1454\n",
      "Epoch 340/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5094 - acc: 0.4584 - val_loss: 3.3121 - val_acc: 0.1481\n",
      "Epoch 341/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5105 - acc: 0.4567 - val_loss: 3.3599 - val_acc: 0.1406\n",
      "Epoch 342/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5080 - acc: 0.4574 - val_loss: 3.3487 - val_acc: 0.1433\n",
      "Epoch 343/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5096 - acc: 0.4591 - val_loss: 3.3752 - val_acc: 0.1372\n",
      "Epoch 344/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5091 - acc: 0.4579 - val_loss: 3.3342 - val_acc: 0.1402\n",
      "Epoch 345/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5126 - acc: 0.4539 - val_loss: 3.3410 - val_acc: 0.1420\n",
      "Epoch 346/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.5206 - acc: 0.4533 - val_loss: 3.3324 - val_acc: 0.1496\n",
      "Epoch 347/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5068 - acc: 0.4615 - val_loss: 3.3735 - val_acc: 0.1452\n",
      "Epoch 348/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5123 - acc: 0.4558 - val_loss: 3.3736 - val_acc: 0.1343\n",
      "Epoch 349/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5049 - acc: 0.4620 - val_loss: 3.3841 - val_acc: 0.1443\n",
      "Epoch 350/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5037 - acc: 0.4593 - val_loss: 3.3276 - val_acc: 0.1404\n",
      "Epoch 351/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5059 - acc: 0.4611 - val_loss: 3.3349 - val_acc: 0.1452\n",
      "Epoch 352/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5075 - acc: 0.4632 - val_loss: 3.3555 - val_acc: 0.1446\n",
      "Epoch 353/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5062 - acc: 0.4628 - val_loss: 3.3831 - val_acc: 0.1409\n",
      "Epoch 354/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5045 - acc: 0.4615 - val_loss: 3.3611 - val_acc: 0.1467\n",
      "Epoch 355/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5071 - acc: 0.4596 - val_loss: 3.3702 - val_acc: 0.1435\n",
      "Epoch 356/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4995 - acc: 0.4598 - val_loss: 3.3778 - val_acc: 0.1363\n",
      "Epoch 357/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5030 - acc: 0.4627 - val_loss: 3.3480 - val_acc: 0.1435\n",
      "Epoch 358/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.5015 - acc: 0.4623 - val_loss: 3.3756 - val_acc: 0.1400\n",
      "Epoch 359/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4942 - acc: 0.4675 - val_loss: 3.3565 - val_acc: 0.1419\n",
      "Epoch 360/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4922 - acc: 0.4674 - val_loss: 3.3784 - val_acc: 0.1406\n",
      "Epoch 361/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4941 - acc: 0.4672 - val_loss: 3.3515 - val_acc: 0.1491\n",
      "Epoch 362/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4951 - acc: 0.4674 - val_loss: 3.3989 - val_acc: 0.1446\n",
      "Epoch 363/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4967 - acc: 0.4658 - val_loss: 3.3612 - val_acc: 0.1426\n",
      "Epoch 364/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4958 - acc: 0.4639 - val_loss: 3.3730 - val_acc: 0.1446\n",
      "Epoch 365/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4953 - acc: 0.4690 - val_loss: 3.3729 - val_acc: 0.1450\n",
      "Epoch 366/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4901 - acc: 0.4699 - val_loss: 3.3577 - val_acc: 0.1437\n",
      "Epoch 367/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4960 - acc: 0.4666 - val_loss: 3.4112 - val_acc: 0.1420\n",
      "Epoch 368/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4914 - acc: 0.4711 - val_loss: 3.3866 - val_acc: 0.1443\n",
      "Epoch 369/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4884 - acc: 0.4675 - val_loss: 3.4242 - val_acc: 0.1439\n",
      "Epoch 370/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4902 - acc: 0.4692 - val_loss: 3.3845 - val_acc: 0.1446\n",
      "Epoch 371/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4909 - acc: 0.4686 - val_loss: 3.4422 - val_acc: 0.1398\n",
      "Epoch 372/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.4808 - acc: 0.4716 - val_loss: 3.4100 - val_acc: 0.1461\n",
      "Epoch 373/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4808 - acc: 0.4737 - val_loss: 3.3814 - val_acc: 0.1435\n",
      "Epoch 374/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4840 - acc: 0.4730 - val_loss: 3.3987 - val_acc: 0.1372\n",
      "Epoch 375/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4843 - acc: 0.4749 - val_loss: 3.3931 - val_acc: 0.1411\n",
      "Epoch 376/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4781 - acc: 0.4789 - val_loss: 3.3538 - val_acc: 0.1461\n",
      "Epoch 377/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4798 - acc: 0.4775 - val_loss: 3.4074 - val_acc: 0.1383\n",
      "Epoch 378/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4806 - acc: 0.4745 - val_loss: 3.4207 - val_acc: 0.1404\n",
      "Epoch 379/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4810 - acc: 0.4755 - val_loss: 3.3908 - val_acc: 0.1435\n",
      "Epoch 380/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4818 - acc: 0.4719 - val_loss: 3.4018 - val_acc: 0.1417\n",
      "Epoch 381/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4760 - acc: 0.4787 - val_loss: 3.3766 - val_acc: 0.1463\n",
      "Epoch 382/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4772 - acc: 0.4795 - val_loss: 3.4410 - val_acc: 0.1380\n",
      "Epoch 383/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4707 - acc: 0.4820 - val_loss: 3.4494 - val_acc: 0.1389\n",
      "Epoch 384/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4761 - acc: 0.4774 - val_loss: 3.4123 - val_acc: 0.1443\n",
      "Epoch 385/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4800 - acc: 0.4752 - val_loss: 3.4181 - val_acc: 0.1433\n",
      "Epoch 386/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4715 - acc: 0.4799 - val_loss: 3.4153 - val_acc: 0.1450\n",
      "Epoch 387/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4755 - acc: 0.4776 - val_loss: 3.4104 - val_acc: 0.1415\n",
      "Epoch 388/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4768 - acc: 0.4802 - val_loss: 3.4159 - val_acc: 0.1428\n",
      "Epoch 389/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4689 - acc: 0.4839 - val_loss: 3.4210 - val_acc: 0.1419\n",
      "Epoch 390/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4760 - acc: 0.4755 - val_loss: 3.4043 - val_acc: 0.1480\n",
      "Epoch 391/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4653 - acc: 0.4792 - val_loss: 3.4371 - val_acc: 0.1437\n",
      "Epoch 392/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.4704 - acc: 0.4795 - val_loss: 3.4348 - val_acc: 0.1417\n",
      "Epoch 393/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4645 - acc: 0.4866 - val_loss: 3.4180 - val_acc: 0.1424\n",
      "Epoch 394/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4670 - acc: 0.4808 - val_loss: 3.4477 - val_acc: 0.1415\n",
      "Epoch 395/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4782 - acc: 0.4778 - val_loss: 3.4178 - val_acc: 0.1448\n",
      "Epoch 396/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4686 - acc: 0.4844 - val_loss: 3.4295 - val_acc: 0.1469\n",
      "Epoch 397/400\n",
      "13200/13200 [==============================] - 33s - loss: 1.4730 - acc: 0.4788 - val_loss: 3.4078 - val_acc: 0.1381\n",
      "Epoch 398/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4609 - acc: 0.4865 - val_loss: 3.4690 - val_acc: 0.1437\n",
      "Epoch 399/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4623 - acc: 0.4865 - val_loss: 3.4592 - val_acc: 0.1420\n",
      "Epoch 400/400\n",
      "13200/13200 [==============================] - 32s - loss: 1.4586 - acc: 0.4903 - val_loss: 3.4228 - val_acc: 0.1396\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "results = []\n",
    "for subject_name in data_set_locations:\n",
    "    file_name = r'C:\\Users\\ORI\\Documents\\Thesis\\dataset_all\\{0}'.format(subject_name)\n",
    "    gcd_res = readCompleteMatFile(file_name)\n",
    "    subject_results = dict()\n",
    "    for i in range(1):\n",
    "        model.set_weights(original_weights)\n",
    "#         model_mlp.set_weights(original_weights_mlp)\n",
    "        down_sample_param = 8\n",
    "        train_data, train_tags = create_train_data(gcd_res, down_samples_param=down_sample_param)\n",
    "        print train_tags.shape\n",
    "        \n",
    "#         target_location = np.where(np.all([gcd_res['train_mode'] == 1,gcd_res['target'] == 1 ], axis = 0))[0]\n",
    "#         non_target_location = np.where(np.all([gcd_res['train_mode'] == 1 ], axis = 0))[0]\n",
    "        # np.abs(non_target_location - 13).argmin()\n",
    "#         print target_location[0]\n",
    "#         print len(target_location)\n",
    "#         distance_from_target =  np.zeros((len(target_location), len(non_target_location)))\n",
    "#         for i, item in enumerate(target_location) :\n",
    "#             distance_from_target[i,:] = non_target_location - item\n",
    "            \n",
    "\n",
    "#         minimal_distance_from_target = np.zeros_like(non_target_location)\n",
    "#         for i, item in enumerate(np.abs(distance_from_target).argmin(axis=0).astype(np.int)) :\n",
    "#             minimal_distance_from_target[i] =  non_target_location[i] - target_location[item]\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        shuffeled_samples, suffule_tags = shuffle(train_data, train_tags, random_state=0)\n",
    "        \n",
    "        \n",
    "        test_data, test_tags = create_letter_test_data(gcd_res, down_samples_param=down_sample_param)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for epoch_i in range(1):\n",
    "            model.fit(stats.zscore(shuffeled_samples, axis=1), suffule_tags,\n",
    "                      nb_epoch=400, show_accuracy=True, verbose=1,validation_data=(stats.zscore(test_data, axis=1), test_tags))\n",
    "            \n",
    "            \n",
    "            \n",
    "# #             predicted_res = model.predict(stats.zscore(train_data, axis=1))\n",
    "# #             print train_data.shape\n",
    "            \n",
    "# #             print_true_vs_predict(train_tags, predicted_res)\n",
    "# #             print distance_from_target.shape\n",
    "# #             print predicted_res.shape\n",
    "# #             plt.scatter(minimal_distance_from_target, predicted_res[:,1],alpha=0.1)\n",
    "# #             hist_gaps = dict()\n",
    "# #             for gaps_i, gap in enumerate(range(-30,31)):\n",
    "# #                 hist_gaps[gaps_i] = predicted_res[:,1][minimal_distance_from_target == gap].sum()\n",
    "# #             print hist_gaps\n",
    "# #             plt.show()\n",
    "#             test_data_gcd, test_target_gcd = create_evaluation_data(gcd_res, down_samples_param=down_sample_param)\n",
    "\n",
    "#             test_prediction = model.predict(stats.zscore(test_data_gcd, axis=1), verbose=1)\n",
    "#             test_prediction_mlp = model_mlp.predict(stats.zscore(test_data_gcd, axis=1), verbose=1)\n",
    "            \n",
    "\n",
    "# #             x, y, _ = roc_curve(test_target_gcd, test_prediction[:, 1])\n",
    "# #             x_mlp, y_mlp, _ = roc_curve(test_target_gcd, test_prediction_mlp[:, 1])\n",
    "            \n",
    "            \n",
    "#             # This is the ROC curve\n",
    "#             # plt.plot(x, y)\n",
    "            \n",
    "            \n",
    "#             # plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "#             auc_score = roc_auc_score(test_target_gcd, test_prediction[:, 1])\n",
    "#             auc_score_mlp = roc_auc_score(test_target_gcd, test_prediction_mlp[:, 1])\n",
    "#             print \"auc_score:{0}\".format(auc_score)\n",
    "#             sub_gcd_res = create_data_for_compare_by_repetition(file_name)\n",
    "#             # sub_gcd_res = dict(train_trial=gcd_res['train_trial'][gcd_res['train_mode'] != 1],\n",
    "#             # train_block=gcd_res['train_block'][gcd_res['train_mode'] != 1],\n",
    "#             # stimulus=gcd_res['stimulus'][gcd_res['train_mode'] != 1])\n",
    "\n",
    "#             _, _, gt_data_for_sum = create_target_table(sub_gcd_res, test_target_gcd)\n",
    "#             _, _, actual_data_for_sum = create_target_table(sub_gcd_res, test_prediction[:, 1])\n",
    "#             _, _, actual_data_for_sum_mlp = create_target_table(sub_gcd_res, test_prediction_mlp[:, 1])\n",
    "#             subject_results[i] = dict(test_prediction=test_prediction, \n",
    "#                                       auc_score=auc_score,\n",
    "#                                       acc_by_rep=accuracy_by_repetition(actual_data_for_sum, gt_data_for_sum, number_of_repetition=10))\n",
    "\n",
    "#             print \"accuracy_by_repetition {0}\".format(\n",
    "#                 accuracy_by_repetition(actual_data_for_sum, gt_data_for_sum, number_of_repetition=10))\n",
    "            \n",
    "#             print \"accuracy_by_repetition_mlp {0}\".format(\n",
    "#                 accuracy_by_repetition(actual_data_for_sum_mlp, gt_data_for_sum, number_of_repetition=10))\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    break\n",
    "    results.append(dict(subject_name=subject_name, subject_results=subject_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400L, 1L)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [  31 5400]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7ec0db3ab00f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.pyc\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \"\"\"\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[1;32m--> 176\u001b[1;33m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [  31 5400]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "validation_data=(stats.zscore(test_data, axis=1), test_tags)\n",
    "prediction_res = model.predict(stats.zscore(test_data, axis=1))\n",
    "print to_categorical(prediction_res.max(axis=1)).shape\n",
    "test_tags.shape\n",
    "confusion_matrix(test_tags, to_categorical(prediction_res.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "confusion_matrix(test_tags.argmax(axis=1), prediction_res.argmax(axis=1))\n",
    "ABC_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z','-','.','!','<']\n",
    "df = pd.DataFrame(confusion_matrix(test_tags.argmax(axis=1), prediction_res.argmax(axis=1)), columns=ABC_list, index=ABC_list)\n",
    "# df.set_index(ABC_list)\n",
    "df.to_excel(r'c:\\temp\\letters.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(ABC_list))\n",
    "    plt.xticks(tick_marks, ABC_list, rotation=45)\n",
    "    plt.yticks(tick_marks, ABC_list)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqMAAALJCAYAAACA8CpUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4bc96780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(confusion_matrix(test_tags.argmax(axis=1), prediction_res.argmax(axis=1)))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-.!<"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
