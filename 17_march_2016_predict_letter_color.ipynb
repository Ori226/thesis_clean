{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 740M (CNMeM is disabled, CuDNN not available)\n",
      "C:\\Anaconda\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'ORI'\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "# I should learn how to load libraries in a more elegant way\n",
    "ABC_list =   ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z','_','.','!','<']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "color_dictionary = dict()\n",
    "for letter_i, letter in enumerate(ABC_list):\n",
    "    print letter_i\n",
    "    #     red (fRyGk<)\n",
    "    if letter in list('fRyGk<'.lower()):\n",
    "        color_dictionary[letter_i+1] = 0\n",
    "    #     blue (iSwc_N)\n",
    "    if letter in list('iSwc_N'.lower()):\n",
    "        color_dictionary[letter_i+1] = 1\n",
    "    #     green (TBMqAH), \n",
    "    if letter in list('TBMqAH'.lower()):\n",
    "        color_dictionary[letter_i+1] = 2\n",
    "    #     black (LdvOz.).\n",
    "    if letter in list('LdvOz.'.lower()):\n",
    "        color_dictionary[letter_i+1] = 3\n",
    "    #     white (pJUX!E)\n",
    "    if letter in list('pJUX!E'.lower()):\n",
    "        color_dictionary[letter_i+1] = 4\n",
    "    \n",
    "    \n",
    "print color_dictionary[27]\n",
    "\n",
    "sys.path.append(r'C:\\Users\\ORI\\Documents\\IDC-non-sync\\Thesis\\PythonApplication1\\OriKerasExtension')\n",
    "#import OriKerasExtension\n",
    "import ThesisHelper\n",
    "#reload(OriKerasExtension)\n",
    "reload(ThesisHelper)\n",
    "from   ThesisHelper import LoadSingleSubjectPython, readCompleteMatFile, ExtractDataVer4\n",
    "import P300Prediction\n",
    "reload(P300Prediction)\n",
    "from P300Prediction import accuracy_by_repetition, create_target_table\n",
    "\n",
    "\n",
    "sys.path.append(r'C:\\Users\\ORI\\Documents\\IDC-non-sync\\Thesis\\PythonApplication1\\OriKerasExtension')\n",
    "#import OriKerasExtension\n",
    "import ThesisHelper\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import P300Prediction\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "#reload(OriKerasExtension)\n",
    "reload(ThesisHelper)\n",
    "from ThesisHelper import LoadSingleSubjectPython, readCompleteMatFile, ExtractDataVer4\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "reload(P300Prediction)\n",
    "from P300Prediction import accuracy_by_repetition, create_target_table\n",
    "\n",
    "\n",
    "# [all_target, all_non_target] = LoadSingleSubjectPython(r'C:\\Users\\ORI\\Documents\\Thesis\\dataset_all\\RSVP_Color116msVPfat.mat')\n",
    "\n",
    "\n",
    "\n",
    "# all_samples = np.vstack((all_target,all_non_target))\n",
    "\n",
    "\n",
    "# '''\n",
    "# Create the tagging column\n",
    "# '''\n",
    "# all_tags = np.vstack((np.ones((all_target.shape[0],1)), np.zeros((all_non_target.shape[0],1))))\n",
    "\n",
    "\n",
    "\n",
    "# from OriKerasExtension.OriKerasExtension import DebugLSTM\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from  keras.regularizers import WeightRegularizer\n",
    "\n",
    "'''\n",
    "define the neural network model:\n",
    "'''\n",
    "\n",
    "\n",
    "def create_compile_cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    number_of_time_stamps = 20\n",
    "    number_of_out_channels = 10\n",
    "    number_of_in_channels = 55\n",
    "    length_of_time_axe_mask = 10\n",
    "\n",
    "    model.add(Convolution2D(nb_filter=10,\n",
    "                            nb_col=number_of_out_channels,\n",
    "                            nb_row=1,\n",
    "                            input_shape=(1, number_of_time_stamps, number_of_in_channels),\n",
    "                            border_mode='same',\n",
    "                            init='glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(1, number_of_in_channels)))\n",
    "    model.add(\n",
    "        Convolution2D(nb_filter=number_of_out_channels, nb_row=6, nb_col=1, border_mode='same', init='glorot_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(20, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "    return model\n",
    "\n",
    "def create_compile_lstm_model():\n",
    "\n",
    "    '''\n",
    "    define the neural network model:\n",
    "    '''\n",
    "    model_lstm = Sequential()\n",
    "\n",
    "    model_lstm.add(LSTM(input_dim=55, output_dim=20,return_sequences=True))\n",
    "    model_lstm.add(Dropout(0.3))\n",
    "    model_lstm.add(LSTM(input_dim=20, output_dim=20,return_sequences=False))\n",
    "    model_lstm.add(Dense(2, W_regularizer=l2(0.06)))\n",
    "    model_lstm.add(Activation('softmax'))\n",
    "    model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "def create_compile_lstm_model_letter():\n",
    "\n",
    "    '''\n",
    "    define the neural network model:\n",
    "    '''\n",
    "    model_lstm = Sequential()\n",
    "\n",
    "    model_lstm.add(LSTM(input_dim=55, output_dim=20,return_sequences=True))\n",
    "    model_lstm.add(Dropout(0.01))\n",
    "    model_lstm.add(LSTM(input_dim=20, output_dim=20,return_sequences=False))\n",
    "#     model_lstm.add(Dropout(0.01))\n",
    "#     model_lstm.add(LSTM(input_dim=20, output_dim=20,return_sequences=False))\n",
    "    model_lstm.add(Dense(5, W_regularizer=l2(0.006)))\n",
    "    model_lstm.add(Activation('softmax'))\n",
    "    model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "\n",
    "def create_compile_dense_model():\n",
    "\n",
    "    '''\n",
    "    define the neural network model:\n",
    "    '''\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(keras.layers.core.Flatten(input_shape=(55,100)))\n",
    "    model_lstm.add(Dense(input_dim=55*100, output_dim=30 , W_regularizer=l2(0.06)))\n",
    "    model_lstm.add(Activation('tanh'))\n",
    "    model_lstm.add(Dense(2))\n",
    "    model_lstm.add(Activation('softmax'))\n",
    "    model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "def create_small_compile_dense_model():\n",
    "\n",
    "    '''\n",
    "    define the neural network model:\n",
    "    '''\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(keras.layers.core.Flatten(input_shape=(55,25)))\n",
    "    model_lstm.add(Dense(input_dim=55*25, output_dim=20 ))\n",
    "    model_lstm.add(Dropout(0.3))\n",
    "    model_lstm.add(Activation('tanh'))\n",
    "    model_lstm.add(Dense(output_dim=20 , W_regularizer=l2(0.06)))\n",
    "    model_lstm.add(Activation('tanh'))\n",
    "    model_lstm.add(Dense(2))\n",
    "    model_lstm.add(Activation('softmax'))\n",
    "    model_lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "# def down_sample_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_evaluation_data(gcd_res, down_samples_param):\n",
    "#     gcd_res = readCompleteMatFile(file_name)\n",
    "    data_for_eval = ExtractDataVer4(gcd_res['all_relevant_channels'], gcd_res['marker_positions'], gcd_res['target'],\n",
    "                                    -200, 800)\n",
    "    # print  data_for_eval\n",
    "\n",
    "    temp_data_for_eval = downsample_data(data_for_eval[0], data_for_eval[0].shape[1], down_samples_param)\n",
    "\n",
    "    test_data_gcd, test_target_gcd = temp_data_for_eval[gcd_res['train_mode'] != 1], data_for_eval[1][\n",
    "        gcd_res['train_mode'] != 1]\n",
    "    return test_data_gcd, test_target_gcd\n",
    "\n",
    "\n",
    "def downsample_data(data, number_of_original_samples, down_samples_param):\n",
    "\n",
    "\n",
    "    new_number_of_time_stamps = number_of_original_samples / down_samples_param\n",
    "\n",
    "\n",
    "    # print  data_for_eval\n",
    "    temp_data_for_eval = np.zeros((data.shape[0], new_number_of_time_stamps, data.shape[2]))\n",
    "\n",
    "    for new_i, i in enumerate(range(0, number_of_original_samples, down_samples_param)):\n",
    "        temp_data_for_eval[:, new_i, :] = np.mean(data[:, range(i, (i + down_samples_param)), :], axis=1)\n",
    "    return temp_data_for_eval\n",
    "\n",
    "def get_color_from_stimuli(stimulus_vetor, color_dictionary):\n",
    "    return [color_dictionary[x] for x in stimulus_vetor]\n",
    "#     red (fRyGk<), \n",
    "#     white (pJUX!E),\n",
    "#     blue (iSwc_N), \n",
    "#     green (TBMqAH), \n",
    "#     black (LdvOz.).\n",
    "    pass\n",
    "\n",
    "def create_train_data(gcd_res, down_samples_param):\n",
    "    all_positive_train = []\n",
    "    all_negative_train = []\n",
    "\n",
    "    last_time_stamp = 800\n",
    "    fist_time_stamp = -200\n",
    "\n",
    "\n",
    "    data_for_eval = ExtractDataVer4(gcd_res['all_relevant_channels'], gcd_res['marker_positions'],\n",
    "                                    gcd_res['target'], fist_time_stamp, last_time_stamp)\n",
    "    \n",
    "    print data_for_eval[0].shape\n",
    "    temp_data_for_eval = downsample_data(data_for_eval[0],data_for_eval[0].shape[1], down_samples_param)\n",
    "\n",
    "    all_data = temp_data_for_eval[np.all([gcd_res['train_mode'] != 3], axis=0)]\n",
    "    \n",
    "\n",
    "    \n",
    "    categorical_tags = to_categorical(get_color_from_stimuli(gcd_res['stimulus'][gcd_res['train_mode'] != 3], color_dictionary))\n",
    "    indexes = range(len(categorical_tags))\n",
    "    print \"len(categorical_tags) {0}\".format(len(categorical_tags))\n",
    "\n",
    "    shuffeled_samples, suffule_tags = (all_data, categorical_tags)\n",
    "    return shuffeled_samples, suffule_tags\n",
    "\n",
    "def create_letter_test_data(gcd_res, down_samples_param):\n",
    "    all_positive_train = []\n",
    "    all_negative_train = []\n",
    "\n",
    "    last_time_stamp = 800\n",
    "    fist_time_stamp = -200\n",
    "\n",
    "\n",
    "    data_for_eval = ExtractDataVer4(gcd_res['all_relevant_channels'], gcd_res['marker_positions'],\n",
    "                                    gcd_res['target'], fist_time_stamp, last_time_stamp)\n",
    "    \n",
    "    print data_for_eval[0].shape\n",
    "    temp_data_for_eval = downsample_data(data_for_eval[0],data_for_eval[0].shape[1], down_samples_param)\n",
    "\n",
    "    all_data = temp_data_for_eval[np.all([gcd_res['train_mode'] == 3], axis=0)]\n",
    "    \n",
    "    categorical_tags = to_categorical(get_color_from_stimuli(gcd_res['stimulus'][gcd_res['train_mode'] == 3], color_dictionary))\n",
    "    \n",
    "#     categorical_tags = to_categorical(gcd_res['stimulus'][gcd_res['train_mode'] == 3])\n",
    "    indexes = range(len(categorical_tags))\n",
    "    print \"len(categorical_tags) {0}\".format(len(categorical_tags))\n",
    "\n",
    "    shuffeled_samples, suffule_tags = (all_data, categorical_tags)\n",
    "    return shuffeled_samples, suffule_tags\n",
    "\n",
    "\n",
    "def create_data_for_compare_by_repetition(file_name):\n",
    "    gcd_res = readCompleteMatFile(file_name)\n",
    "    sub_gcd_res = dict(train_trial=gcd_res['train_trial'][gcd_res['train_mode'] != 1],\n",
    "                       train_block=gcd_res['train_block'][gcd_res['train_mode'] != 1],\n",
    "                       stimulus=gcd_res['stimulus'][gcd_res['train_mode'] != 1])\n",
    "    return sub_gcd_res\n",
    "\n",
    "#shuffeled_samples, suffule_tags = create_train_data(file_name=None, down_samples_param=5)\n",
    "# shuffeled_samples, suffule_tags = create_train_data(file_name=None, down_samples_param=20)\n",
    "# original_weights_mlp = model_mlp.get_weights()\n",
    "\n",
    "data_set_locations = [\"RSVP_Color116msVPicr.mat\",\n",
    "                      \"RSVP_Color116msVPpia.mat\",\n",
    "                      \"RSVP_Color116msVPfat.mat\",\n",
    "                      \"RSVP_Color116msVPgcb.mat\",\n",
    "                      \"RSVP_Color116msVPgcc.mat\",\n",
    "                      \"RSVP_Color116msVPgcd.mat\",\n",
    "                      \"RSVP_Color116msVPgcf.mat\",\n",
    "                      \"RSVP_Color116msVPgcg.mat\",\n",
    "                      \"RSVP_Color116msVPgch.mat\",\n",
    "                      \"RSVP_Color116msVPiay.mat\",\n",
    "                      \"RSVP_Color116msVPicn.mat\"];\n",
    "\n",
    "# data_set_locations = [\"RSVP_Color116msVPgcd.mat\"]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for subject_name in data_set_locations:\n",
    "#     file_name = r'C:\\Users\\ORI\\Documents\\Thesis\\dataset_all\\{0}'.format(subject_name)\n",
    "#     gcd_res = readCompleteMatFile(file_name)\n",
    "\n",
    "def print_true_vs_predict(true_val, predicted_val):\n",
    "    import matplotlib.cm as cm        \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(true_val[:,1].reshape(-1,30), cmap=cm.Greys_r, interpolation='none', aspect='auto')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(predicted_val[:,1].reshape(-1,30), cmap=cm.Greys_r, interpolation='none', aspect='auto')\n",
    "    plt.show()\n",
    "\n",
    "# def calculate_proximity_to_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13200L, 5L)\n"
     ]
    }
   ],
   "source": [
    "gcd_res = readCompleteMatFile(r'C:\\Users\\ORI\\Documents\\Thesis\\dataset_all\\{0}'.format(\"RSVP_Color116msVPicr.mat\"))\n",
    "gcd_res.keys()\n",
    "# to_categorical(gcd_res['stimulus'])\n",
    "temp =to_categorical(get_color_from_stimuli(gcd_res['stimulus'][gcd_res['train_mode'] != 3], color_dictionary))\n",
    "print temp.shape\n",
    "\n",
    "# create_train_data(gcd_res,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_compile_lstm_model_letter()\n",
    "# model_mlp = create_small_compile_dense_model()\n",
    "original_weights = model.get_weights()\n",
    "# original_weights_mlp = model_mlp.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18600L, 200L, 55L)\n",
      "len(categorical_tags) 13200\n",
      "(13200L, 5L)\n",
      "(18600L, 200L, 55L)\n",
      "len(categorical_tags) 5400\n",
      "Train on 13200 samples, validate on 5400 samples\n",
      "Epoch 1/100\n",
      "13200/13200 [==============================] - 21s - loss: 1.6540 - acc: 0.2075 - val_loss: 1.6073 - val_acc: 0.2228\n",
      "Epoch 2/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.6266 - acc: 0.2411 - val_loss: 1.5947 - val_acc: 0.2483\n",
      "Epoch 3/100\n",
      "13200/13200 [==============================] - 21s - loss: 1.5976 - acc: 0.2861 - val_loss: 1.5700 - val_acc: 0.2813\n",
      "Epoch 4/100\n",
      "13200/13200 [==============================] - 21s - loss: 1.5458 - acc: 0.3338 - val_loss: 1.5205 - val_acc: 0.3372\n",
      "Epoch 5/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.4836 - acc: 0.3726 - val_loss: 1.4768 - val_acc: 0.3696\n",
      "Epoch 6/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.4175 - acc: 0.4152 - val_loss: 1.4332 - val_acc: 0.3967\n",
      "Epoch 7/100\n",
      "13200/13200 [==============================] - 23s - loss: 1.3575 - acc: 0.4539 - val_loss: 1.4005 - val_acc: 0.4187\n",
      "Epoch 8/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.3038 - acc: 0.4864 - val_loss: 1.3881 - val_acc: 0.4302\n",
      "Epoch 9/100\n",
      "13200/13200 [==============================] - 23s - loss: 1.2547 - acc: 0.5168 - val_loss: 1.3782 - val_acc: 0.4467\n",
      "Epoch 10/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.2110 - acc: 0.5434 - val_loss: 1.3758 - val_acc: 0.4559\n",
      "Epoch 11/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.1666 - acc: 0.5681 - val_loss: 1.3690 - val_acc: 0.4550\n",
      "Epoch 12/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.1240 - acc: 0.5950 - val_loss: 1.3143 - val_acc: 0.4806\n",
      "Epoch 13/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.0762 - acc: 0.6223 - val_loss: 1.2913 - val_acc: 0.5067\n",
      "Epoch 14/100\n",
      "13200/13200 [==============================] - 22s - loss: 1.0308 - acc: 0.6477 - val_loss: 1.2639 - val_acc: 0.5244\n",
      "Epoch 15/100\n",
      "13200/13200 [==============================] - 21s - loss: 0.9831 - acc: 0.6745 - val_loss: 1.2512 - val_acc: 0.5454\n",
      "Epoch 16/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.9345 - acc: 0.6949 - val_loss: 1.1965 - val_acc: 0.5672\n",
      "Epoch 17/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.8847 - acc: 0.7189 - val_loss: 1.1742 - val_acc: 0.5863\n",
      "Epoch 18/100\n",
      "13200/13200 [==============================] - 21s - loss: 0.8315 - acc: 0.7423 - val_loss: 1.1676 - val_acc: 0.5956\n",
      "Epoch 19/100\n",
      "13200/13200 [==============================] - 21s - loss: 0.7847 - acc: 0.7646 - val_loss: 1.1073 - val_acc: 0.6169\n",
      "Epoch 20/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.7383 - acc: 0.7834 - val_loss: 1.0952 - val_acc: 0.6283\n",
      "Epoch 21/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.6947 - acc: 0.7987 - val_loss: 1.1003 - val_acc: 0.6446\n",
      "Epoch 22/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.6573 - acc: 0.8139 - val_loss: 1.0184 - val_acc: 0.6596\n",
      "Epoch 23/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.6189 - acc: 0.8288 - val_loss: 0.9729 - val_acc: 0.6689\n",
      "Epoch 24/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.5858 - acc: 0.8423 - val_loss: 0.9769 - val_acc: 0.6874\n",
      "Epoch 25/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.5567 - acc: 0.8515 - val_loss: 0.9566 - val_acc: 0.6915\n",
      "Epoch 26/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.5297 - acc: 0.8607 - val_loss: 0.9185 - val_acc: 0.7083\n",
      "Epoch 27/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.5025 - acc: 0.8722 - val_loss: 0.9413 - val_acc: 0.7085\n",
      "Epoch 28/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.4786 - acc: 0.8795 - val_loss: 0.9004 - val_acc: 0.7204\n",
      "Epoch 29/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.4605 - acc: 0.8860 - val_loss: 0.9230 - val_acc: 0.7235\n",
      "Epoch 30/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.4428 - acc: 0.8935 - val_loss: 0.9170 - val_acc: 0.7278\n",
      "Epoch 31/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.4249 - acc: 0.8983 - val_loss: 0.8700 - val_acc: 0.7343\n",
      "Epoch 32/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.4090 - acc: 0.9058 - val_loss: 0.8539 - val_acc: 0.7409\n",
      "Epoch 33/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.3983 - acc: 0.9086 - val_loss: 0.8527 - val_acc: 0.7409\n",
      "Epoch 34/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.3802 - acc: 0.9173 - val_loss: 0.8368 - val_acc: 0.7446\n",
      "Epoch 35/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.3645 - acc: 0.9227 - val_loss: 0.8561 - val_acc: 0.7481\n",
      "Epoch 36/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.3517 - acc: 0.9261 - val_loss: 0.9133 - val_acc: 0.7439\n",
      "Epoch 37/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.3406 - acc: 0.9296 - val_loss: 0.8259 - val_acc: 0.7580\n",
      "Epoch 38/100\n",
      "13200/13200 [==============================] - 24s - loss: 0.3290 - acc: 0.9341 - val_loss: 0.8170 - val_acc: 0.7656\n",
      "Epoch 39/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.3211 - acc: 0.9350 - val_loss: 0.8136 - val_acc: 0.7663\n",
      "Epoch 40/100\n",
      "13200/13200 [==============================] - 24s - loss: 0.3118 - acc: 0.9370 - val_loss: 0.8823 - val_acc: 0.7598\n",
      "Epoch 41/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.3023 - acc: 0.9392 - val_loss: 0.8392 - val_acc: 0.7617\n",
      "Epoch 42/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.2921 - acc: 0.9452 - val_loss: 0.8333 - val_acc: 0.7661\n",
      "Epoch 43/100\n",
      "13200/13200 [==============================] - 23s - loss: 0.2912 - acc: 0.9433 - val_loss: 0.8262 - val_acc: 0.7644\n",
      "Epoch 44/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2765 - acc: 0.9479 - val_loss: 0.8505 - val_acc: 0.7665\n",
      "Epoch 45/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2727 - acc: 0.9491 - val_loss: 0.8119 - val_acc: 0.7750\n",
      "Epoch 46/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2636 - acc: 0.9536 - val_loss: 0.7896 - val_acc: 0.7743\n",
      "Epoch 47/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2582 - acc: 0.9564 - val_loss: 0.8222 - val_acc: 0.7739\n",
      "Epoch 48/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2519 - acc: 0.9555 - val_loss: 0.8267 - val_acc: 0.7696\n",
      "Epoch 49/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2452 - acc: 0.9567 - val_loss: 0.7946 - val_acc: 0.7785\n",
      "Epoch 50/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2435 - acc: 0.9585 - val_loss: 0.8116 - val_acc: 0.7746\n",
      "Epoch 51/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2327 - acc: 0.9608 - val_loss: 0.8316 - val_acc: 0.7761\n",
      "Epoch 52/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2303 - acc: 0.9631 - val_loss: 0.7883 - val_acc: 0.7819\n",
      "Epoch 53/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2263 - acc: 0.9623 - val_loss: 0.8693 - val_acc: 0.7713\n",
      "Epoch 54/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2181 - acc: 0.9659 - val_loss: 0.8324 - val_acc: 0.7754\n",
      "Epoch 55/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2180 - acc: 0.9648 - val_loss: 0.8511 - val_acc: 0.7748\n",
      "Epoch 56/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2102 - acc: 0.9682 - val_loss: 0.8009 - val_acc: 0.7743\n",
      "Epoch 57/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2082 - acc: 0.9685 - val_loss: 0.8253 - val_acc: 0.7735\n",
      "Epoch 58/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2055 - acc: 0.9689 - val_loss: 0.8352 - val_acc: 0.7794\n",
      "Epoch 59/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.2003 - acc: 0.9697 - val_loss: 0.8404 - val_acc: 0.7841\n",
      "Epoch 60/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1979 - acc: 0.9705 - val_loss: 0.8266 - val_acc: 0.7811\n",
      "Epoch 61/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1966 - acc: 0.9714 - val_loss: 0.8380 - val_acc: 0.7819\n",
      "Epoch 62/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1905 - acc: 0.9739 - val_loss: 0.8352 - val_acc: 0.7830\n",
      "Epoch 63/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1885 - acc: 0.9728 - val_loss: 0.8290 - val_acc: 0.7835\n",
      "Epoch 64/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1891 - acc: 0.9720 - val_loss: 0.8242 - val_acc: 0.7824\n",
      "Epoch 65/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1864 - acc: 0.9750 - val_loss: 0.8153 - val_acc: 0.7852\n",
      "Epoch 66/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1781 - acc: 0.9759 - val_loss: 0.8181 - val_acc: 0.7863\n",
      "Epoch 67/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1782 - acc: 0.9752 - val_loss: 0.8943 - val_acc: 0.7757\n",
      "Epoch 68/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1784 - acc: 0.9758 - val_loss: 0.7637 - val_acc: 0.7885\n",
      "Epoch 69/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1731 - acc: 0.9775 - val_loss: 0.8163 - val_acc: 0.7835\n",
      "Epoch 70/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1717 - acc: 0.9777 - val_loss: 0.8500 - val_acc: 0.7863\n",
      "Epoch 71/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1687 - acc: 0.9788 - val_loss: 0.7930 - val_acc: 0.7863\n",
      "Epoch 72/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1714 - acc: 0.9769 - val_loss: 0.8003 - val_acc: 0.7863\n",
      "Epoch 73/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1649 - acc: 0.9801 - val_loss: 0.8217 - val_acc: 0.7907\n",
      "Epoch 74/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1641 - acc: 0.9808 - val_loss: 0.8402 - val_acc: 0.7859\n",
      "Epoch 75/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1622 - acc: 0.9798 - val_loss: 0.8560 - val_acc: 0.7824\n",
      "Epoch 76/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1609 - acc: 0.9806 - val_loss: 0.8235 - val_acc: 0.7904\n",
      "Epoch 77/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1590 - acc: 0.9805 - val_loss: 0.8193 - val_acc: 0.7802\n",
      "Epoch 78/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1591 - acc: 0.9804 - val_loss: 0.7832 - val_acc: 0.7881\n",
      "Epoch 79/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1569 - acc: 0.9814 - val_loss: 0.8245 - val_acc: 0.7894\n",
      "Epoch 80/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1516 - acc: 0.9843 - val_loss: 0.8228 - val_acc: 0.7844\n",
      "Epoch 81/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1539 - acc: 0.9833 - val_loss: 0.7963 - val_acc: 0.7911\n",
      "Epoch 82/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1525 - acc: 0.9832 - val_loss: 0.8275 - val_acc: 0.7931\n",
      "Epoch 83/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1495 - acc: 0.9842 - val_loss: 0.8056 - val_acc: 0.7939\n",
      "Epoch 84/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1496 - acc: 0.9833 - val_loss: 0.8145 - val_acc: 0.7924\n",
      "Epoch 85/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1498 - acc: 0.9833 - val_loss: 0.8322 - val_acc: 0.7935\n",
      "Epoch 86/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1430 - acc: 0.9861 - val_loss: 0.7967 - val_acc: 0.7906\n",
      "Epoch 87/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1447 - acc: 0.9860 - val_loss: 0.8188 - val_acc: 0.7846\n",
      "Epoch 88/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1431 - acc: 0.9858 - val_loss: 0.8162 - val_acc: 0.7950\n",
      "Epoch 89/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1421 - acc: 0.9867 - val_loss: 0.8186 - val_acc: 0.7952\n",
      "Epoch 90/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1410 - acc: 0.9859 - val_loss: 0.7959 - val_acc: 0.7948\n",
      "Epoch 91/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1392 - acc: 0.9867 - val_loss: 0.8158 - val_acc: 0.7941\n",
      "Epoch 92/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1361 - acc: 0.9878 - val_loss: 0.8180 - val_acc: 0.7933\n",
      "Epoch 93/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1382 - acc: 0.9871 - val_loss: 0.8401 - val_acc: 0.7874\n",
      "Epoch 94/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1345 - acc: 0.9880 - val_loss: 0.8599 - val_acc: 0.7881\n",
      "Epoch 95/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1321 - acc: 0.9889 - val_loss: 0.8001 - val_acc: 0.7989\n",
      "Epoch 96/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1318 - acc: 0.9883 - val_loss: 0.8368 - val_acc: 0.7967\n",
      "Epoch 97/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1331 - acc: 0.9871 - val_loss: 0.7940 - val_acc: 0.7998\n",
      "Epoch 98/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1343 - acc: 0.9873 - val_loss: 0.8228 - val_acc: 0.7911\n",
      "Epoch 99/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1297 - acc: 0.9895 - val_loss: 0.7977 - val_acc: 0.7952\n",
      "Epoch 100/100\n",
      "13200/13200 [==============================] - 22s - loss: 0.1280 - acc: 0.9902 - val_loss: 0.8260 - val_acc: 0.7957\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "results = []\n",
    "for subject_name in data_set_locations:\n",
    "    file_name = r'C:\\Users\\ORI\\Documents\\Thesis\\dataset_all\\{0}'.format(subject_name)\n",
    "    gcd_res = readCompleteMatFile(file_name)\n",
    "    subject_results = dict()\n",
    "    for i in range(1):\n",
    "        model.set_weights(original_weights)\n",
    "#         model_mlp.set_weights(original_weights_mlp)\n",
    "        down_sample_param = 8\n",
    "        train_data, train_tags = create_train_data(gcd_res, down_samples_param=down_sample_param)\n",
    "        print train_tags.shape\n",
    "        \n",
    "#         target_location = np.where(np.all([gcd_res['train_mode'] == 1,gcd_res['target'] == 1 ], axis = 0))[0]\n",
    "#         non_target_location = np.where(np.all([gcd_res['train_mode'] == 1 ], axis = 0))[0]\n",
    "        # np.abs(non_target_location - 13).argmin()\n",
    "#         print target_location[0]\n",
    "#         print len(target_location)\n",
    "#         distance_from_target =  np.zeros((len(target_location), len(non_target_location)))\n",
    "#         for i, item in enumerate(target_location) :\n",
    "#             distance_from_target[i,:] = non_target_location - item\n",
    "            \n",
    "\n",
    "#         minimal_distance_from_target = np.zeros_like(non_target_location)\n",
    "#         for i, item in enumerate(np.abs(distance_from_target).argmin(axis=0).astype(np.int)) :\n",
    "#             minimal_distance_from_target[i] =  non_target_location[i] - target_location[item]\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        shuffeled_samples, suffule_tags = shuffle(train_data, train_tags, random_state=0)\n",
    "        \n",
    "        \n",
    "        test_data, test_tags = create_letter_test_data(gcd_res, down_samples_param=down_sample_param)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for epoch_i in range(1):\n",
    "            model.fit(stats.zscore(shuffeled_samples, axis=1), suffule_tags,\n",
    "                      nb_epoch=100, show_accuracy=True, verbose=1,validation_data=(stats.zscore(test_data, axis=1), test_tags))\n",
    "            \n",
    "            \n",
    "            \n",
    "# #             predicted_res = model.predict(stats.zscore(train_data, axis=1))\n",
    "# #             print train_data.shape\n",
    "            \n",
    "# #             print_true_vs_predict(train_tags, predicted_res)\n",
    "# #             print distance_from_target.shape\n",
    "# #             print predicted_res.shape\n",
    "# #             plt.scatter(minimal_distance_from_target, predicted_res[:,1],alpha=0.1)\n",
    "# #             hist_gaps = dict()\n",
    "# #             for gaps_i, gap in enumerate(range(-30,31)):\n",
    "# #                 hist_gaps[gaps_i] = predicted_res[:,1][minimal_distance_from_target == gap].sum()\n",
    "# #             print hist_gaps\n",
    "# #             plt.show()\n",
    "#             test_data_gcd, test_target_gcd = create_evaluation_data(gcd_res, down_samples_param=down_sample_param)\n",
    "\n",
    "#             test_prediction = model.predict(stats.zscore(test_data_gcd, axis=1), verbose=1)\n",
    "#             test_prediction_mlp = model_mlp.predict(stats.zscore(test_data_gcd, axis=1), verbose=1)\n",
    "            \n",
    "\n",
    "# #             x, y, _ = roc_curve(test_target_gcd, test_prediction[:, 1])\n",
    "# #             x_mlp, y_mlp, _ = roc_curve(test_target_gcd, test_prediction_mlp[:, 1])\n",
    "            \n",
    "            \n",
    "#             # This is the ROC curve\n",
    "#             # plt.plot(x, y)\n",
    "            \n",
    "            \n",
    "#             # plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "#             auc_score = roc_auc_score(test_target_gcd, test_prediction[:, 1])\n",
    "#             auc_score_mlp = roc_auc_score(test_target_gcd, test_prediction_mlp[:, 1])\n",
    "#             print \"auc_score:{0}\".format(auc_score)\n",
    "#             sub_gcd_res = create_data_for_compare_by_repetition(file_name)\n",
    "#             # sub_gcd_res = dict(train_trial=gcd_res['train_trial'][gcd_res['train_mode'] != 1],\n",
    "#             # train_block=gcd_res['train_block'][gcd_res['train_mode'] != 1],\n",
    "#             # stimulus=gcd_res['stimulus'][gcd_res['train_mode'] != 1])\n",
    "\n",
    "#             _, _, gt_data_for_sum = create_target_table(sub_gcd_res, test_target_gcd)\n",
    "#             _, _, actual_data_for_sum = create_target_table(sub_gcd_res, test_prediction[:, 1])\n",
    "#             _, _, actual_data_for_sum_mlp = create_target_table(sub_gcd_res, test_prediction_mlp[:, 1])\n",
    "#             subject_results[i] = dict(test_prediction=test_prediction, \n",
    "#                                       auc_score=auc_score,\n",
    "#                                       acc_by_rep=accuracy_by_repetition(actual_data_for_sum, gt_data_for_sum, number_of_repetition=10))\n",
    "\n",
    "#             print \"accuracy_by_repetition {0}\".format(\n",
    "#                 accuracy_by_repetition(actual_data_for_sum, gt_data_for_sum, number_of_repetition=10))\n",
    "            \n",
    "#             print \"accuracy_by_repetition_mlp {0}\".format(\n",
    "#                 accuracy_by_repetition(actual_data_for_sum_mlp, gt_data_for_sum, number_of_repetition=10))\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    break\n",
    "    results.append(dict(subject_name=subject_name, subject_results=subject_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('c:\\tmp\\letters_model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400L, 1L)\n",
      "(5400L,)\n",
      "[0 4 1 ..., 4 1 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[863,  57,  62,  63,  35],\n",
       "       [ 67, 858,  41,  80,  34],\n",
       "       [ 70,  32, 872,  38,  68],\n",
       "       [ 40,  84,  34, 845,  77],\n",
       "       [ 41,  29,  77,  74, 859]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "validation_data=(stats.zscore(test_data, axis=1), test_tags)\n",
    "prediction_res = model.predict(stats.zscore(test_data, axis=1))\n",
    "print to_categorical(prediction_res.max(axis=1)).shape\n",
    "print test_tags.argmax(axis=1).shape\n",
    "print prediction_res.argmax(axis=1)\n",
    "confusion_matrix(test_tags.argmax(axis=1),prediction_res.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "confusion_matrix(test_tags.argmax(axis=1), prediction_res.argmax(axis=1))\n",
    "ABC_list = ['reb', 'blue', 'green', 'black', 'white']\n",
    "df = pd.DataFrame(confusion_matrix(test_tags.argmax(axis=1), prediction_res.argmax(axis=1)), columns=ABC_list, index=ABC_list)\n",
    "# df.set_index(ABC_list)\n",
    "df.to_excel(r'c:\\temp\\letters_colors.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(ABC_list))\n",
    "    plt.xticks(tick_marks, ABC_list, rotation=45)\n",
    "    plt.yticks(tick_marks, ABC_list)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqwAAALICAYAAAC6p6J8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3a59b128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(confusion_matrix(test_tags.argmax(axis=1), prediction_res.argmax(axis=1)))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-.!<"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
